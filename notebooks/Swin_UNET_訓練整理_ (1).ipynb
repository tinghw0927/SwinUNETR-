{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 環境設定\n",
        "\n"
      ],
      "metadata": {
        "id": "QvMbGLUl1p7R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xW0sziQmccB",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd02571-0938-49e6-ce8c-989cc1fbb6b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m137.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} --version\n",
        "!{sys.executable} -m pip install -U numpy==1.26.4\n",
        "import os\n",
        "os._exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install -U ray==2.5.0\n",
        "import ray\n",
        "print(ray.__version__)"
      ],
      "metadata": {
        "id": "H_wpXy-PmfX0",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "235c0f57-5775-42ae-9330-3dd3df9cc18d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray==2.5.0\n",
            "  Downloading ray-2.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from ray==2.5.0) (25.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.5.0) (8.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.5.0) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.5.0) (4.24.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.5.0) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.5.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray==2.5.0) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ray==2.5.0) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.5.0) (1.4.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.5.0) (1.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray==2.5.0) (2.32.3)\n",
            "Collecting grpcio<=1.51.3,>=1.42.0 (from ray==2.5.0)\n",
            "  Downloading grpcio-1.51.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from ray==2.5.0) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal->ray==2.5.0) (4.14.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.5.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.5.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.5.0) (0.26.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray==2.5.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray==2.5.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray==2.5.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray==2.5.0) (2025.7.14)\n",
            "Downloading ray-2.5.0-cp311-cp311-manylinux2014_x86_64.whl (56.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.51.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: grpcio, ray\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.73.1\n",
            "    Uninstalling grpcio-1.73.1:\n",
            "      Successfully uninstalled grpcio-1.73.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires grpcio>=1.71.2, but you have grpcio 1.51.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed grpcio-1.51.3 ray-2.5.0\n",
            "2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install -U monai==1.2.0\n",
        "import monai\n",
        "print(monai.__version__)"
      ],
      "metadata": {
        "id": "zpiX0eC9mgu8",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81bb6c91-4511-445d-b904-c69c90569881"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting monai==1.2.0\n",
            "  Downloading monai-1.2.0-202306081546-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from monai==1.2.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from monai==1.2.0) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.2.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.2.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.2.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.2.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.9->monai==1.2.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.9->monai==1.2.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.9->monai==1.2.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9->monai==1.2.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9->monai==1.2.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9->monai==1.2.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9->monai==1.2.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9->monai==1.2.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9->monai==1.2.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.2.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.2.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9->monai==1.2.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.2.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->monai==1.2.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->monai==1.2.0) (3.0.2)\n",
            "Downloading monai-1.2.0-202306081546-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m134.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m136.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, monai\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed monai-1.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ml_collections"
      ],
      "metadata": {
        "id": "W0Hxz1pomiUE",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec71808-7854-4ec5-a49e-59c7833d41d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml_collections\n",
            "  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from ml_collections) (6.0.2)\n",
            "Downloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml_collections\n",
            "Successfully installed ml_collections-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kairaun/Sam.git"
      ],
      "metadata": {
        "id": "ndve6GSqmj2c",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7d9c56-7462-4ba4-d5b6-1ebf27a4e960"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sam'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./Sam\n",
        "! ls\n",
        "! head pyproject.toml\n",
        "!pip install -e .\n",
        "import os\n",
        "os._exit(0)"
      ],
      "metadata": {
        "id": "WVlxui6FmljT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97723ac6-2f72-4648-a44b-7560992d621b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Sam\n",
            "pyproject.toml\n",
            "[build-system]\n",
            "requires = [\n",
            "    \"setuptools>=62.3.0,<75.9\",  \n",
            "    \"torch>=2.5.1\",\n",
            "    ]\n",
            "build-backend = \"setuptools.build_meta\"\n",
            "Obtaining file:///content/Sam\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: UNKNOWN\n",
            "  Building editable for UNKNOWN (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for UNKNOWN: filename=unknown-0.0.0-0.editable-py3-none-any.whl size=2557 sha256=28547f110541d3585d3a5b6a1d432ffa0ffead287b87b4e2649c46eef0430987\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1sz97hgb/wheels/4d/dd/8b/46a6ee24a860a2484f39a9b119ad19ebd742e574a063e7b881\n",
            "Successfully built UNKNOWN\n",
            "Installing collected packages: UNKNOWN\n",
            "Successfully installed UNKNOWN-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install -U --no-deps monailabel\n",
        "import monailabel\n",
        "print(monailabel.__version__)"
      ],
      "metadata": {
        "id": "VAI44nFAmnGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9517ed5d-d739-496f-8598-52f4a7263d19",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting monailabel\n",
            "  Downloading monailabel-0.8.5-202411252313-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading monailabel-0.8.5-202411252313-py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: monailabel\n",
            "Successfully installed monailabel-0.8.5\n",
            "0.8.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "h1cViHvT20Uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade0e7cf-39ab-4b5b-8e13-5e30a0343fdc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"pydantic<2.0\""
      ],
      "metadata": {
        "id": "GhMRAITYnAr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9367b153-cf7e-49f2-cde4-8b2dce7ab9d2",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic<2.0\n",
            "  Downloading pydantic-1.10.24-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.0) (4.14.1)\n",
            "Downloading pydantic-1.10.24-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydantic\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.7\n",
            "    Uninstalling pydantic-2.11.7:\n",
            "      Successfully uninstalled pydantic-2.11.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "monailabel 0.8.5 requires bcrypt>=4.1.2, which is not installed.\n",
            "monailabel 0.8.5 requires dicomweb-client[gcp]>=0.59.1, which is not installed.\n",
            "monailabel 0.8.5 requires expiring_dict>=1.1.0, which is not installed.\n",
            "monailabel 0.8.5 requires expiringdict>=1.2.2, which is not installed.\n",
            "monailabel 0.8.5 requires girder-client>=3.2.3, which is not installed.\n",
            "monailabel 0.8.5 requires ninja>=1.11.1.1, which is not installed.\n",
            "monailabel 0.8.5 requires numpymaxflow>=0.0.7, which is not installed.\n",
            "monailabel 0.8.5 requires passlib>=1.7.4, which is not installed.\n",
            "monailabel 0.8.5 requires pydantic-settings>=2.2.1, which is not installed.\n",
            "monailabel 0.8.5 requires pydicom>=2.4.4, which is not installed.\n",
            "monailabel 0.8.5 requires pydicom-seg>=0.4.1, which is not installed.\n",
            "monailabel 0.8.5 requires pynetdicom>=2.0.2, which is not installed.\n",
            "monailabel 0.8.5 requires pynrrd>=1.0.0, which is not installed.\n",
            "monailabel 0.8.5 requires python-dotenv>=1.0.1, which is not installed.\n",
            "monailabel 0.8.5 requires sam2>=0.4.1; python_version >= \"3.10\", which is not installed.\n",
            "monailabel 0.8.5 requires schedule>=1.2.1, which is not installed.\n",
            "monailabel 0.8.5 requires timeloop>=1.0.2, which is not installed.\n",
            "monailabel 0.8.5 requires watchdog>=4.0.0, which is not installed.\n",
            "monailabel 0.8.5 requires monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]>=1.4.0, but you have monai 1.2.0 which is incompatible.\n",
            "monailabel 0.8.5 requires pydantic>=2.7.0, but you have pydantic 1.10.24 which is incompatible.\n",
            "albumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 1.10.24 which is incompatible.\n",
            "google-genai 1.25.0 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.24 which is incompatible.\n",
            "gradio 5.31.0 requires pydantic<2.12,>=2.0, but you have pydantic 1.10.24 which is incompatible.\n",
            "langchain 0.3.26 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.24 which is incompatible.\n",
            "langchain-core 0.3.68 requires pydantic>=2.7.4, but you have pydantic 1.10.24 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.24 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-1.10.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the project\n",
        "!git clone https://github.com/kairaun/CardiacSegV2.git"
      ],
      "metadata": {
        "id": "8X5Bn1pcmo-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58706e97-fae5-4fd9-f49c-c5ec2e57bf58",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CardiacSegV2'...\n",
            "remote: Enumerating objects: 641, done.\u001b[K\n",
            "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 641 (delta 98), reused 68 (delta 68), pack-reused 531 (from 1)\u001b[K\n",
            "Receiving objects: 100% (641/641), 17.31 MiB | 17.22 MiB/s, done.\n",
            "Resolving deltas: 100% (207/207), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Environment\n",
        "! sudo chmod 700 /content/CardiacSegV2/setup.sh\n",
        "! /content/CardiacSegV2/setup.sh\n",
        "! pip install gdown==4.6.0"
      ],
      "metadata": {
        "id": "08FZegO6mwCM",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ddfe060-531a-4c98-f13f-c55b5830259d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,286 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,201 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,081 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,842 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,281 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [44.9 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,901 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,571 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:25 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [60.9 kB]\n",
            "Get:26 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,510 kB]\n",
            "Fetched 38.1 MB in 5s (8,130 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "194 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsm6 is already the newest version (2:1.2.3-1build2).\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 194 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.12.1+cu113 (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0, 2.9.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.12.1+cu113\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting gdown==4.6.0\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown==4.6.0) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown==4.6.0) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (2025.7.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (1.7.1)\n",
            "Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 5.2.0\n",
            "    Uninstalling gdown-5.2.0:\n",
            "      Successfully uninstalled gdown-5.2.0\n",
            "Successfully installed gdown-4.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the code\n",
        "! sed -i '8c \\    os.makedirs(\\\"/content/CardiacSegV2/\\\"+data_dir, exist_ok=True)' /content/CardiacSegV2/setup_dir.py\n",
        "! sed -i '20c \\        os.makedirs(\\\"/content/CardiacSegV2/\\\"+model_exp_dir, exist_ok=True)' /content/CardiacSegV2/setup_dir.py\n",
        "! sed -i '29c \\        os.makedirs(\"/content/CardiacSegV2/\"+model_exp_dir, exist_ok=True)' /content/CardiacSegV2/setup_dir.py\n",
        "! sed -i '3c sys.path.append(\\\"/content/CardiacSegV2\\\")' /content/CardiacSegV2/expers/infer.py\n",
        "! sed -i '3c sys.path.append(\\\"/content/CardiacSegV2\\\")' /content/CardiacSegV2/expers/tune.py\n",
        "! sed -i '36c ray.init(runtime_env={\\\"working_dir\\\": \\\"/content/CardiacSegV2\\\"})' /content/CardiacSegV2/expers/tune.py"
      ],
      "metadata": {
        "id": "dfq4d56hm0ac"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python /content/CardiacSegV2/setup_dir.py"
      ],
      "metadata": {
        "id": "f5L_3Gvrmzqz",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e65dff9a-ab75-41aa-8112-d0b73faa6d88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir data: dataset/chgh\n",
            "mkdir exp: exps/exps/unet3d/chgh/tune_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 下載訓練資料集"
      ],
      "metadata": {
        "id": "KIgn3huKOrt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "CtWvxoe0MiJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47f09e8c-4bc2-40a4-e4da-aa0bf8d6c777"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CardiacSegV2  drive  Sam  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 安裝 PyTorch\n",
        "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --extra-index-url https://download.pytorch.org/whl/cu118\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8W14IiRPh3s",
        "outputId": "88823df0-219c-4a26-8660-459bbf8d7b80",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.16.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.1.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m138.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.3.2)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0) (2025.7.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Installing collected packages: triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed torch-2.1.0+cu118 torchaudio-2.1.0+cu118 torchvision-0.16.0+cu118 triton-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 掛載 Google 雲端硬碟\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 設定 ZIP 檔案路徑\n",
        "zip_path = \"/content/drive/MyDrive/training_label.zip\"\n",
        "\n",
        "# 建立目標資料夾\n",
        "!mkdir -p /content/CardiacSegV2/dataset/chgh/\n",
        "\n",
        "# 檢查 ZIP 檔案是否存在\n",
        "import os\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"找不到 ZIP 檔案，請檢查檔名或路徑是否正確。\")\n",
        "else:\n",
        "    print(\"找到 ZIP 檔案：\", zip_path)\n",
        "\n",
        "# 解壓縮\n",
        "print(\"\\n開始解壓縮...\")\n",
        "!unzip -q -o \"{zip_path}\" -d \"/content/CardiacSegV2/dataset/chgh/\"\n",
        "print(\"解壓縮完成\")\n",
        "\n",
        "\n",
        "# 檢查結果\n",
        "print(\"\\n 解壓縮完成，以下是資料夾內容：\")\n",
        "!ls -R /content/CardiacSegV2/dataset/chgh | head -n 150\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBOkk8xMdDQB",
        "outputId": "0548664f-f2e0-4202-e36a-1e41ee7b4920",
        "collapsed": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "找到 ZIP 檔案： /content/drive/MyDrive/training_label.zip\n",
            "\n",
            "開始解壓縮...\n",
            "解壓縮完成\n",
            "\n",
            " 解壓縮完成，以下是資料夾內容：\n",
            "/content/CardiacSegV2/dataset/chgh:\n",
            "__MACOSX\n",
            "training_image\n",
            "training_label\n",
            "\n",
            "/content/CardiacSegV2/dataset/chgh/__MACOSX:\n",
            "training_label\n",
            "\n",
            "/content/CardiacSegV2/dataset/chgh/__MACOSX/training_label:\n",
            "41_training_image_01\n",
            "41_training_image_02\n",
            "41_training_image_03\n",
            "41_training_label\n",
            "\n",
            "/content/CardiacSegV2/dataset/chgh/__MACOSX/training_label/41_training_image_01:\n",
            "\n",
            "/content/CardiacSegV2/dataset/chgh/__MACOSX/training_label/41_training_image_02:\n",
            "\n",
            "/content/CardiacSegV2/dataset/chgh/__MACOSX/training_label/41_training_image_03:\n",
            "\n",
            "/content/CardiacSegV2/dataset/chgh/__MACOSX/training_label/41_training_label:\n",
            "\n",
            "/content/CardiacSegV2/dataset/chgh/training_image:\n",
            "\n",
            "/content/CardiacSegV2/dataset/chgh/training_label:\n",
            "41_training_image_01\n",
            "41_training_image_02\n",
            "41_training_image_03\n",
            "41_training_label\n",
            "\n",
            "/content/CardiacSegV2/dataset/chgh/training_label/41_training_image_01:\n",
            "patient0001.nii.gz\n",
            "patient0002.nii.gz\n",
            "patient0003.nii.gz\n",
            "patient0004.nii.gz\n",
            "patient0005.nii.gz\n",
            "patient0006.nii.gz\n",
            "patient0007.nii.gz\n",
            "patient0008.nii.gz\n",
            "patient0009.nii.gz\n",
            "patient0010.nii.gz\n",
            "patient0011.nii.gz\n",
            "patient0012.nii.gz\n",
            "patient0013.nii.gz\n",
            "patient0014.nii.gz\n",
            "patient0015.nii.gz\n",
            "patient0016.nii.gz\n",
            "patient0017.nii.gz\n",
            "patient0018.nii.gz\n",
            "patient0019.nii.gz\n",
            "patient0020.nii.gz\n",
            "\n",
            "/content/CardiacSegV2/dataset/chgh/training_label/41_training_image_02:\n",
            "patient0021.nii.gz\n",
            "patient0022.nii.gz\n",
            "patient0023.nii.gz\n",
            "patient0024.nii.gz\n",
            "patient0025.nii.gz\n",
            "patient0026.nii.gz\n",
            "patient0027.nii.gz\n",
            "patient0028.nii.gz\n",
            "patient0029.nii.gz\n",
            "patient0030.nii.gz\n",
            "patient0031.nii.gz\n",
            "patient0032.nii.gz\n",
            "patient0033.nii.gz\n",
            "patient0034.nii.gz\n",
            "patient0035.nii.gz\n",
            "patient0036.nii.gz\n",
            "patient0037.nii.gz\n",
            "patient0038.nii.gz\n",
            "patient0039.nii.gz\n",
            "patient0040.nii.gz\n",
            "\n",
            "/content/CardiacSegV2/dataset/chgh/training_label/41_training_image_03:\n",
            "patient0041.nii.gz\n",
            "patient0042.nii.gz\n",
            "patient0043.nii.gz\n",
            "patient0044.nii.gz\n",
            "patient0045.nii.gz\n",
            "patient0046.nii.gz\n",
            "patient0047.nii.gz\n",
            "patient0048.nii.gz\n",
            "patient0049.nii.gz\n",
            "patient0050.nii.gz\n",
            "\n",
            "/content/CardiacSegV2/dataset/chgh/training_label/41_training_label:\n",
            "patient0001_gt.nii.gz\n",
            "patient0002_gt.nii.gz\n",
            "patient0003_gt.nii.gz\n",
            "patient0004_gt.nii.gz\n",
            "patient0005_gt.nii.gz\n",
            "patient0006_gt.nii.gz\n",
            "patient0007_gt.nii.gz\n",
            "patient0008_gt.nii.gz\n",
            "patient0009_gt.nii.gz\n",
            "patient0010_gt.nii.gz\n",
            "patient0011_gt.nii.gz\n",
            "patient0012_gt.nii.gz\n",
            "patient0013_gt.nii.gz\n",
            "patient0014_gt.nii.gz\n",
            "patient0015_gt.nii.gz\n",
            "patient0016_gt.nii.gz\n",
            "patient0017_gt.nii.gz\n",
            "patient0018_gt.nii.gz\n",
            "patient0019_gt.nii.gz\n",
            "patient0020_gt.nii.gz\n",
            "patient0021_gt.nii.gz\n",
            "patient0022_gt.nii.gz\n",
            "patient0023_gt.nii.gz\n",
            "patient0024_gt.nii.gz\n",
            "patient0025_gt.nii.gz\n",
            "patient0026_gt.nii.gz\n",
            "patient0027_gt.nii.gz\n",
            "patient0028_gt.nii.gz\n",
            "patient0029_gt.nii.gz\n",
            "patient0030_gt.nii.gz\n",
            "patient0031_gt.nii.gz\n",
            "patient0032_gt.nii.gz\n",
            "patient0033_gt.nii.gz\n",
            "patient0034_gt.nii.gz\n",
            "patient0035_gt.nii.gz\n",
            "patient0036_gt.nii.gz\n",
            "patient0037_gt.nii.gz\n",
            "patient0038_gt.nii.gz\n",
            "patient0039_gt.nii.gz\n",
            "patient0040_gt.nii.gz\n",
            "patient0041_gt.nii.gz\n",
            "patient0042_gt.nii.gz\n",
            "patient0043_gt.nii.gz\n",
            "patient0044_gt.nii.gz\n",
            "patient0045_gt.nii.gz\n",
            "patient0046_gt.nii.gz\n",
            "patient0047_gt.nii.gz\n",
            "patient0048_gt.nii.gz\n",
            "patient0049_gt.nii.gz\n",
            "patient0050_gt.nii.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 整理資料夾結構\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "base_path = \"/content/CardiacSegV2/dataset/chgh\"\n",
        "\n",
        "# 檢查是否已經整理過\n",
        "if os.path.exists(f\"{base_path}/training_image\") and os.path.exists(f\"{base_path}/training_label\"):\n",
        "    # 檢查資料夾是否有檔案\n",
        "    img_files = len([f for f in os.listdir(f\"{base_path}/training_image\") if f.endswith('.nii.gz')]) if os.path.exists(f\"{base_path}/training_image\") else 0\n",
        "    lbl_files = len([f for f in os.listdir(f\"{base_path}/training_label\") if f.endswith('.nii.gz')]) if os.path.exists(f\"{base_path}/training_label\") else 0\n",
        "\n",
        "    if img_files > 0 and lbl_files > 0:\n",
        "        print(f\"\\n資料已整理完成\")\n",
        "        print(f\"   影像檔案: {img_files} 個\")\n",
        "        print(f\"   標籤檔案: {lbl_files} 個\")\n",
        "        print(\"\\n跳過整理步驟,直接進行配對\")\n",
        "    else:\n",
        "        print(\"\\n資料夾存在但為空,將重新整理\")\n",
        "        need_reorganize = True\n",
        "else:\n",
        "    need_reorganize = True\n",
        "\n",
        "# 如果需要整理\n",
        "if 'need_reorganize' in locals() and need_reorganize:\n",
        "    print(\"\\n開始整理檔案...\")\n",
        "\n",
        "    # 定位資料來源\n",
        "    if os.path.exists(f\"{base_path}/training_label/41_training_image_01\"):\n",
        "        source_base = f\"{base_path}/training_label\"\n",
        "    elif os.path.exists(f\"{base_path}/41_training_image_01\"):\n",
        "        source_base = base_path\n",
        "    else:\n",
        "        source_base = None\n",
        "        print(\"找不到資料來源資料夾!\")\n",
        "\n",
        "    if source_base:\n",
        "        print(f\"   資料來源: {source_base}\")\n",
        "\n",
        "        # 建立目標資料夾\n",
        "        target_image_dir = f\"{base_path}/training_image\"\n",
        "        target_label_dir = f\"{base_path}/training_label_new\"\n",
        "\n",
        "        # 清理並建立資料夾\n",
        "        for dir_path in [target_image_dir, target_label_dir]:\n",
        "            if os.path.exists(dir_path):\n",
        "                shutil.rmtree(dir_path)\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "        # 搬移圖片檔案\n",
        "        moved_images = 0\n",
        "        for i in range(1, 4):\n",
        "            src_dir = f\"{source_base}/41_training_image_0{i}\"\n",
        "            if os.path.exists(src_dir):\n",
        "                files = [f for f in os.listdir(src_dir) if f.endswith('.nii.gz')]\n",
        "                for f in files:\n",
        "                    shutil.move(os.path.join(src_dir, f), target_image_dir)\n",
        "                    moved_images += 1\n",
        "                shutil.rmtree(src_dir)\n",
        "                print(f\"   搬移 {len(files)} 個檔案從 41_training_image_0{i}\")\n",
        "        # 搬移標籤檔案\n",
        "        src_label_dir = f\"{source_base}/41_training_label\"\n",
        "        moved_labels = 0\n",
        "        if os.path.exists(src_label_dir):\n",
        "            files = [f for f in os.listdir(src_label_dir) if f.endswith('.nii.gz')]\n",
        "            for f in files:\n",
        "                shutil.move(os.path.join(src_label_dir, f), target_label_dir)\n",
        "                moved_labels += 1\n",
        "            shutil.rmtree(src_label_dir)\n",
        "            print(f\"   搬移 {len(files)} 個標籤檔案\")\n",
        "\n",
        "        # 重新命名資料夾\n",
        "        if os.path.exists(f\"{base_path}/training_label\"):\n",
        "            shutil.rmtree(f\"{base_path}/training_label\")\n",
        "        os.rename(target_label_dir, f\"{base_path}/training_label\")\n",
        "\n",
        "        # 刪除 __MACOSX\n",
        "        if os.path.exists(f\"{base_path}/__MACOSX\"):\n",
        "            shutil.rmtree(f\"{base_path}/__MACOSX\")\n",
        "\n",
        "        print(f\"\\n整理完成:\")\n",
        "        print(f\"   影像檔案: {moved_images} 個\")\n",
        "        print(f\"   標籤檔案: {moved_labels} 個\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYuMPfPjko_i",
        "outputId": "0d4534a0-e670-437d-9e97-4dd0e7f7c68f",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "資料夾存在但為空,將重新整理\n",
            "\n",
            "開始整理檔案...\n",
            "   資料來源: /content/CardiacSegV2/dataset/chgh/training_label\n",
            "   搬移 20 個檔案從 41_training_image_01\n",
            "   搬移 20 個檔案從 41_training_image_02\n",
            "   搬移 10 個檔案從 41_training_image_03\n",
            "   搬移 50 個標籤檔案\n",
            "\n",
            "整理完成:\n",
            "   影像檔案: 50 個\n",
            "   標籤檔案: 50 個\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 掃描並配對檔案\n",
        "import json\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"掃描檔案\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "data_dir = \"/content/CardiacSegV2/dataset/chgh\"\n",
        "\n",
        "# 掃描檔案\n",
        "images = sorted(glob.glob(f\"{data_dir}/training_image/*.nii.gz\"))\n",
        "labels = sorted(glob.glob(f\"{data_dir}/training_label/*.nii.gz\"))\n",
        "\n",
        "print(f\"\\n影像檔案: {len(images)} 個\")\n",
        "print(f\"標籤檔案: {len(labels)} 個\")\n",
        "\n",
        "if len(images) > 0:\n",
        "    print(f\"\\n影像範例:\")\n",
        "    for img in images[:3]:\n",
        "        print(f\"   - {os.path.basename(img)}\")\n",
        "\n",
        "if len(labels) > 0:\n",
        "    print(f\"\\n標籤範例:\")\n",
        "    for lbl in labels[:3]:\n",
        "        print(f\"   - {os.path.basename(lbl)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPsotYK9lBvx",
        "outputId": "c024c676-1225-4dcb-d051-3fdc0cc64c0e",
        "collapsed": true
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "掃描檔案\n",
            "======================================================================\n",
            "\n",
            "影像檔案: 50 個\n",
            "標籤檔案: 50 個\n",
            "\n",
            "影像範例:\n",
            "   - patient0001.nii.gz\n",
            "   - patient0002.nii.gz\n",
            "   - patient0003.nii.gz\n",
            "\n",
            "標籤範例:\n",
            "   - patient0001_gt.nii.gz\n",
            "   - patient0002_gt.nii.gz\n",
            "   - patient0003_gt.nii.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 配對影像和標籤\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"配對影像和標籤\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "dataset = []\n",
        "unmatched = []\n",
        "\n",
        "# _gt 後綴配對\n",
        "for img in images:\n",
        "    img_name = os.path.basename(img).replace('.nii.gz', '')\n",
        "    label_name = f\"{img_name}_gt.nii.gz\"\n",
        "    label_path = os.path.join(f\"{data_dir}/training_label\", label_name)\n",
        "\n",
        "    if os.path.exists(label_path):\n",
        "        dataset.append({\n",
        "            \"image\": img,\n",
        "            \"label\": label_path\n",
        "        })\n",
        "    else:\n",
        "        unmatched.append(img_name)\n",
        "\n",
        "print(f\"使用 _gt 配對: {len(dataset)} 組\")\n",
        "# 如果配對失敗,直接名稱配對\n",
        "if len(dataset) == 0 and len(images) > 0:\n",
        "    print(\"\\n _gt 配對失敗,嘗試直接名稱配對...\")\n",
        "    dataset = []\n",
        "    unmatched = []\n",
        "\n",
        "    for img in images:\n",
        "        img_name = os.path.basename(img)\n",
        "        label_path = os.path.join(f\"{data_dir}/training_label\", img_name)\n",
        "\n",
        "        if os.path.exists(label_path):\n",
        "            dataset.append({\n",
        "                \"image\": img,\n",
        "                \"label\": label_path\n",
        "            })\n",
        "        else:\n",
        "            unmatched.append(img_name)\n",
        "\n",
        "    print(f\"使用直接配對: {len(dataset)} 組\")\n",
        "# 顯示未配對的檔案\n",
        "if unmatched and len(unmatched) <= 10:\n",
        "    print(f\"\\n 未配對的檔案: {len(unmatched)} 個\")\n",
        "    for name in unmatched:\n",
        "        print(f\"   - {name}\")\n",
        "\n",
        "if len(dataset) == 0:\n",
        "    print(\"\\n錯誤: 無法配對任何檔案!\")\n",
        "    print(\"\\n請檢查檔案命名規則:\")\n",
        "    print(\"影像範例:\")\n",
        "    !ls /content/CardiacSegV2/dataset/chgh/training_image/ | head -n 3\n",
        "    print(\"\\n標籤範例:\")\n",
        "    !ls /content/CardiacSegV2/dataset/chgh/training_label/ | head -n 3\n",
        "else:\n",
        "    print(f\"\\n成功配對: {len(dataset)} 組\")\n",
        "\n",
        "    # 分割資料集\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"分割資料集\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    n_train, n_val, n_test = 9, 3, 3\n",
        "\n",
        "    if len(dataset) < 15:\n",
        "        print(f\" 資料不足 15 組,將調整數量\")\n",
        "        n_test = min(3, len(dataset))\n",
        "        n_val = min(3, len(dataset) - n_test)\n",
        "        n_train = len(dataset) - n_test - n_val\n",
        "        print(f\"   調整為: 訓練={n_train}, 驗證={n_val}, 測試={n_test}\")\n",
        "\n",
        "    # 第一次分割: 測試集\n",
        "    remaining, test_files = train_test_split(\n",
        "        dataset,\n",
        "        test_size=min(n_test, len(dataset)),\n",
        "        random_state=42\n",
        "    )\n",
        "    # 第二次分割: 驗證集\n",
        "    if len(remaining) >= (n_train + n_val):\n",
        "        train_files, val_files = train_test_split(\n",
        "            remaining,\n",
        "            test_size=n_val,\n",
        "            random_state=42\n",
        "        )\n",
        "    else:\n",
        "        n_val_actual = min(n_val, len(remaining) // 2)\n",
        "        train_files, val_files = train_test_split(\n",
        "            remaining,\n",
        "            test_size=n_val_actual,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    # 限制訓練集數量\n",
        "    if len(train_files) > n_train:\n",
        "        train_files = train_files[:n_train]\n",
        "\n",
        "# 建立資料字典\n",
        "    data_dict = {\n",
        "        \"training\": train_files,\n",
        "        \"validation\": val_files,\n",
        "        \"test\": test_files\n",
        "    }\n",
        "\n",
        "    # 儲存 JSON\n",
        "    json_path = f\"{data_dir}/AICUP_training.json\"\n",
        "    with open(json_path, \"w\", encoding='utf-8') as f:\n",
        "        json.dump(data_dict, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# 顯示結果\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"資料集分割完成!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"訓練集 (training):   {len(train_files)} 個樣本\")\n",
        "    print(f\"驗證集 (validation): {len(val_files)} 個樣本\")\n",
        "    print(f\"測試集 (test):       {len(test_files)} 個樣本\")\n",
        "    print(f\"JSON 已儲存: {json_path}\")\n",
        "\n",
        "# 顯示範例\n",
        "    print(\"\\n訓練集範例(前3個):\")\n",
        "    for i, item in enumerate(train_files[:3], 1):\n",
        "        img_name = os.path.basename(item['image'])\n",
        "        lbl_name = os.path.basename(item['label'])\n",
        "        print(f\"  {i}. {img_name} ↔ {lbl_name}\")\n",
        "\n",
        "    print(\"\\n驗證集範例(前3個):\")\n",
        "    for i, item in enumerate(val_files[:3], 1):\n",
        "        img_name = os.path.basename(item['image'])\n",
        "        lbl_name = os.path.basename(item['label'])\n",
        "        print(f\"  {i}. {img_name} ↔ {lbl_name}\")\n",
        "\n",
        "    print(\"\\n測試集範例(前3個):\")\n",
        "    for i, item in enumerate(test_files[:3], 1):\n",
        "        img_name = os.path.basename(item['image'])\n",
        "        lbl_name = os.path.basename(item['label'])\n",
        "        print(f\"  {i}. {img_name} ↔ {lbl_name}\")\n",
        "\n",
        "# 驗證 JSON\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"驗證 JSON 內容\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    with open(json_path, 'r') as f:\n",
        "        verify = json.load(f)\n",
        "\n",
        "    print(f\"JSON 鍵值: {list(verify.keys())}\")\n",
        "\n",
        "    all_ok = True\n",
        "    for key, value in verify.items():\n",
        "        if value is None:\n",
        "            print(f\" {key}: None (錯誤!)\")\n",
        "            all_ok = False\n",
        "        else:\n",
        "            print(f\" {key}: {len(value)} 個樣本\")\n",
        "            if len(value) > 0:\n",
        "                first = value[0]\n",
        "                img_exists = os.path.exists(first['image'])\n",
        "                lbl_exists = os.path.exists(first['label'])\n",
        "                print(f\"   第一筆檔案檢查:\")\n",
        "                print(f\"   {'✅' if img_exists else '❌'} 影像: {os.path.basename(first['image'])}\")\n",
        "                print(f\"   {'✅' if lbl_exists else '❌'} 標籤: {os.path.basename(first['label'])}\")\n",
        "                if not (img_exists and lbl_exists):\n",
        "                    all_ok = False\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    if all_ok:\n",
        "        print(\"第 2 段執行完成!資料準備就緒!\")\n",
        "    else:\n",
        "        print(\" 第 2 段完成,但有部分檢查未通過,請檢查上方訊息\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n掛載 Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\" Google Drive 已掛載!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFCPKX54lJRn",
        "outputId": "2a6fc988-4934-47c2-d806-33e8a7ac0d15",
        "collapsed": true
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "配對影像和標籤\n",
            "======================================================================\n",
            "使用 _gt 配對: 50 組\n",
            "\n",
            "成功配對: 50 組\n",
            "\n",
            "======================================================================\n",
            "分割資料集\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "資料集分割完成!\n",
            "======================================================================\n",
            "訓練集 (training):   9 個樣本\n",
            "驗證集 (validation): 3 個樣本\n",
            "測試集 (test):       3 個樣本\n",
            "JSON 已儲存: /content/CardiacSegV2/dataset/chgh/AICUP_training.json\n",
            "\n",
            "訓練集範例(前3個):\n",
            "  1. patient0043.nii.gz ↔ patient0043_gt.nii.gz\n",
            "  2. patient0028.nii.gz ↔ patient0028_gt.nii.gz\n",
            "  3. patient0041.nii.gz ↔ patient0041_gt.nii.gz\n",
            "\n",
            "驗證集範例(前3個):\n",
            "  1. patient0030.nii.gz ↔ patient0030_gt.nii.gz\n",
            "  2. patient0019.nii.gz ↔ patient0019_gt.nii.gz\n",
            "  3. patient0006.nii.gz ↔ patient0006_gt.nii.gz\n",
            "\n",
            "測試集範例(前3個):\n",
            "  1. patient0014.nii.gz ↔ patient0014_gt.nii.gz\n",
            "  2. patient0040.nii.gz ↔ patient0040_gt.nii.gz\n",
            "  3. patient0031.nii.gz ↔ patient0031_gt.nii.gz\n",
            "\n",
            "======================================================================\n",
            "驗證 JSON 內容\n",
            "======================================================================\n",
            "JSON 鍵值: ['training', 'validation', 'test']\n",
            " training: 9 個樣本\n",
            "   第一筆檔案檢查:\n",
            "   ✅ 影像: patient0043.nii.gz\n",
            "   ✅ 標籤: patient0043_gt.nii.gz\n",
            " validation: 3 個樣本\n",
            "   第一筆檔案檢查:\n",
            "   ✅ 影像: patient0030.nii.gz\n",
            "   ✅ 標籤: patient0030_gt.nii.gz\n",
            " test: 3 個樣本\n",
            "   第一筆檔案檢查:\n",
            "   ✅ 影像: patient0014.nii.gz\n",
            "   ✅ 標籤: patient0014_gt.nii.gz\n",
            "\n",
            "======================================================================\n",
            "第 2 段執行完成!資料準備就緒!\n",
            "======================================================================\n",
            "\n",
            "掛載 Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Google Drive 已掛載!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 重新分割資料集 (40:5:5)\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_dir = \"/content/CardiacSegV2/dataset/chgh\"\n",
        "\n",
        "# 掃描檔案\n",
        "images = sorted(glob.glob(f\"{data_dir}/training_image/*.nii.gz\"))\n",
        "labels = sorted(glob.glob(f\"{data_dir}/training_label/*_gt.nii.gz\"))\n",
        "\n",
        "# 配對\n",
        "dataset = []\n",
        "for img in images:\n",
        "    img_name = os.path.basename(img).replace('.nii.gz', '')\n",
        "    label_name = f\"{img_name}_gt.nii.gz\"\n",
        "    label_path = os.path.join(f\"{data_dir}/training_label\", label_name)\n",
        "    if os.path.exists(label_path):\n",
        "        dataset.append({\"image\": img, \"label\": label_path})\n",
        "\n",
        "print(f\"總共 {len(dataset)} 組資料\")\n",
        "\n",
        "# 分割 40:5:5\n",
        "n_train, n_val, n_test = 40, 5, 5\n",
        "\n",
        "remaining, test_files = train_test_split(dataset, test_size=n_test, random_state=42)\n",
        "train_files, val_files = train_test_split(remaining, test_size=n_val, random_state=42)\n",
        "\n",
        "if len(train_files) > n_train:\n",
        "    train_files = train_files[:n_train]\n",
        "\n",
        "# 儲存 JSON (注意是 train 和 val)\n",
        "data_dict = {\n",
        "    \"train\": train_files,\n",
        "    \"val\": val_files,\n",
        "    \"test\": test_files\n",
        "}\n",
        "\n",
        "json_path = f\"{data_dir}/AICUP_training.json\"\n",
        "with open(json_path, \"w\", encoding='utf-8') as f:\n",
        "    json.dump(data_dict, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n重新分割完成\")\n",
        "print(f\"   訓練集: {len(train_files)} 個\")\n",
        "print(f\"   驗證集: {len(val_files)} 個\")\n",
        "print(f\"   測試集: {len(test_files)} 個\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3A_2PVgK1AJ",
        "outputId": "b6b02e5b-e5c5-4f46-be56-ce0f4c10a4d3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "總共 50 組資料\n",
            "\n",
            "重新分割完成\n",
            "   訓練集: 40 個\n",
            "   驗證集: 5 個\n",
            "   測試集: 5 個\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "設定路徑"
      ],
      "metadata": {
        "id": "MlvSML2gm8Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 設定路徑與驗證 (Checkpoint)\n",
        "\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# 掛載 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ========== 基本設定 ==========\n",
        "workspace_dir = '/content/CardiacSegV2'\n",
        "model_name = 'swinunetr'   # ⭐ 改為 swinunetr\n",
        "data_name = 'chgh'\n",
        "exp_name = 'AICUP_SwinUNETR'\n",
        "data_dict_file_name = 'AICUP_training.json'\n",
        "\n",
        "# Google Drive 備份目錄\n",
        "drive_backup_dir = '/content/drive/MyDrive/CardiacSeg_Checkpoints_SwinUNETR'\n",
        "os.makedirs(drive_backup_dir, exist_ok=True)\n",
        "\n",
        "# 路徑設定\n",
        "root_exp_dir = os.path.join(workspace_dir, 'exps', 'exps', model_name, data_name, 'tune_results')\n",
        "root_data_dir = os.path.join(workspace_dir, 'dataset', data_name)\n",
        "data_dir = root_data_dir\n",
        "data_dicts_json = os.path.join(root_data_dir, data_dict_file_name)\n",
        "\n",
        "# 模型目錄改為 Google Drive\n",
        "model_dir = drive_backup_dir\n",
        "log_dir = os.path.join(workspace_dir, 'logs')\n",
        "eval_dir = os.path.join(workspace_dir, 'evals')\n",
        "\n",
        "# 建立目錄\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "os.makedirs(eval_dir, exist_ok=True)\n",
        "os.makedirs(root_exp_dir, exist_ok=True)\n",
        "\n",
        "# 模型檔案路徑 (儲存在 Drive)\n",
        "best_checkpoint = os.path.join(model_dir, 'best_model.pth')\n",
        "final_checkpoint = os.path.join(model_dir, 'final_model.pth')\n",
        "\n",
        "# 顯示路徑\n",
        "print(\"\\n路徑設定:\")\n",
        "\n",
        "print(f\"工作目錄    : {workspace_dir}\")\n",
        "print(f\"模型名稱    : {model_name}\")\n",
        "print(f\"資料集名稱  : {data_name}\")\n",
        "print(f\"實驗名稱    : {exp_name}\")\n",
        "print(f\"模型目錄  : {model_dir} (Google Drive)\")\n",
        "print(f\"資料目錄    : {data_dir}\")\n",
        "print(f\"JSON 檔案   : {data_dicts_json}\")\n",
        "\n",
        "# 檢查是否有舊的 checkpoint\n",
        "print(\"檢查 Checkpoint 狀態\")\n",
        "\n",
        "\n",
        "checkpoint_exists = False\n",
        "if os.path.exists(final_checkpoint):\n",
        "    checkpoint_exists = True\n",
        "    size_mb = os.path.getsize(final_checkpoint) / (1024**2)\n",
        "    print(f\" 發現已有 checkpoint\")\n",
        "    print(f\"   路徑: {final_checkpoint}\")\n",
        "    print(f\"   大小: {size_mb:.2f} MB\")\n",
        "    print(f\"   將從斷點繼續訓練\")\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "        ckpt = torch.load(final_checkpoint, map_location='cpu')\n",
        "        if 'epoch' in ckpt:\n",
        "            print(f\"   上次訓練到 Epoch: {ckpt['epoch']}\")\n",
        "        if 'best_acc' in ckpt:\n",
        "            print(f\"   最佳 Dice: {ckpt['best_acc']:.4f}\")\n",
        "    except:\n",
        "        pass\n",
        "else:\n",
        "    print(f\"沒有舊 checkpoint,將從頭開始訓練\")\n",
        "\n",
        "# 檢查資料狀態\n",
        "print(\"檢查資料狀態\")\n",
        "\n",
        "all_ready = True\n",
        "\n",
        "if os.path.exists(data_dicts_json):\n",
        "    print(f\"JSON 檔案存在\")\n",
        "    try:\n",
        "        with open(data_dicts_json, 'r') as f:\n",
        "            data_dict = json.load(f)\n",
        "        for key in ['train', 'val', 'test']:\n",
        "            if key in data_dict:\n",
        "                print(f\"   ✅ {key}: {len(data_dict[key])} 個樣本\")\n",
        "            else:\n",
        "                print(f\"   缺少: {key}\")\n",
        "                all_ready = False\n",
        "    except Exception as e:\n",
        "        print(f\"   讀取錯誤: {e}\")\n",
        "        all_ready = False\n",
        "else:\n",
        "    print(\"JSON 檔案不存在,需要執行資料分割\")\n",
        "    all_ready = False\n",
        "\n",
        "training_image_dir = os.path.join(data_dir, \"training_image\")\n",
        "training_label_dir = os.path.join(data_dir, \"training_label\")\n",
        "\n",
        "if os.path.exists(training_image_dir):\n",
        "    image_files = [f for f in os.listdir(training_image_dir) if f.endswith('.nii.gz')]\n",
        "    print(f\"影像資料: {len(image_files)} 個檔案\")\n",
        "else:\n",
        "    print(\"影像資料夾不存在\")\n",
        "    all_ready = False\n",
        "\n",
        "if os.path.exists(training_label_dir):\n",
        "    label_files = [f for f in os.listdir(training_label_dir) if f.endswith('.nii.gz')]\n",
        "    print(f\"標籤資料: {len(label_files)} 個檔案\")\n",
        "else:\n",
        "    print(\"標籤資料夾不存在\")\n",
        "    all_ready = False\n",
        "\n",
        "# 最終結果\n",
        "print(\"\\n\")\n",
        "if all_ready:\n",
        "    print(\"所有檢查通過!可以開始訓練 SwinUNETR!\")\n",
        "    if checkpoint_exists:\n",
        "        print(\"模式: 斷點續訓\")\n",
        "    else:\n",
        "        print(\"模式: 從頭訓練\")\n",
        "else:\n",
        "    print(\"有部分檢查未通過,請先準備資料\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-xf-7uNo6lb",
        "outputId": "d2aed4ab-0924-4390-c8e1-e26e138172c9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "路徑設定:\n",
            "工作目錄    : /content/CardiacSegV2\n",
            "模型名稱    : swinunetr\n",
            "資料集名稱  : chgh\n",
            "實驗名稱    : AICUP_SwinUNETR\n",
            "模型目錄  : /content/drive/MyDrive/CardiacSeg_Checkpoints_SwinUNETR (Google Drive)\n",
            "資料目錄    : /content/CardiacSegV2/dataset/chgh\n",
            "JSON 檔案   : /content/CardiacSegV2/dataset/chgh/AICUP_training.json\n",
            "檢查 Checkpoint 狀態\n",
            " 發現已有 checkpoint\n",
            "   路徑: /content/drive/MyDrive/CardiacSeg_Checkpoints_SwinUNETR/final_model.pth\n",
            "   大小: 719.04 MB\n",
            "   將從斷點繼續訓練\n",
            "   上次訓練到 Epoch: 630\n",
            "   最佳 Dice: 0.8711\n",
            "檢查資料狀態\n",
            "JSON 檔案存在\n",
            "   ✅ train: 40 個樣本\n",
            "   ✅ val: 5 個樣本\n",
            "   ✅ test: 5 個樣本\n",
            "影像資料: 50 個檔案\n",
            "標籤資料: 50 個檔案\n",
            "\n",
            "\n",
            "所有檢查通過!可以開始訓練 SwinUNETR!\n",
            "模式: 斷點續訓\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 開始訓練"
      ],
      "metadata": {
        "id": "4vnpWrjsNdbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#分割資料集\n",
        "data_dir = \"/content/CardiacSegV2/dataset/chgh\"\n",
        "\n",
        "images = sorted(glob.glob(f\"{data_dir}/training_image/*.nii.gz\"))\n",
        "labels = sorted(glob.glob(f\"{data_dir}/training_label/*_gt.nii.gz\"))\n",
        "\n",
        "dataset = []\n",
        "for img in images:\n",
        "    img_name = os.path.basename(img).replace('.nii.gz', '')\n",
        "    label_name = f\"{img_name}_gt.nii.gz\"\n",
        "    label_path = os.path.join(f\"{data_dir}/training_label\", label_name)\n",
        "    if os.path.exists(label_path):\n",
        "        dataset.append({\"image\": img, \"label\": label_path})\n",
        "\n",
        "print(f\"總共 {len(dataset)} 組資料\")\n",
        "\n",
        "n_train, n_val, n_test = 40, 5, 5\n",
        "remaining, test_files = train_test_split(dataset, test_size=n_test, random_state=42)\n",
        "train_files, val_files = train_test_split(remaining, test_size=n_val, random_state=42)\n",
        "\n",
        "if len(train_files) > n_train:\n",
        "    train_files = train_files[:n_train]\n",
        "\n",
        "data_dict = {\n",
        "    \"train\": train_files,\n",
        "    \"val\": val_files,\n",
        "    \"test\": test_files\n",
        "}\n",
        "\n",
        "json_path = f\"{data_dir}/AICUP_training.json\"\n",
        "with open(json_path, \"w\", encoding='utf-8') as f:\n",
        "    json.dump(data_dict, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"   訓練集: {len(train_files)} 個\")\n",
        "print(f\"   驗證集: {len(val_files)} 個\")\n",
        "print(f\"   測試集: {len(test_files)} 個\")\n",
        "\n",
        "# 清理記憶體\n",
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 開始訓練\n",
        "print( \"-\" * 70)\n",
        "print(\"開始訓練\")\n",
        "\n",
        "!pip install -q optuna\n",
        "\n",
        "# training (移除 --lrschedule 和 --warmup_epochs)\n",
        "!python /content/CardiacSegV2/expers/tune.py \\\n",
        "  --tune_mode=\"train\" \\\n",
        "  --exp_name=\"AICUP_swinunetr_v2\" \\\n",
        "  --data_name={data_name} \\\n",
        "  --data_dir={data_dir} \\\n",
        "  --root_exp_dir={root_exp_dir} \\\n",
        "  --model_name={model_name} \\\n",
        "  --model_dir={model_dir} \\\n",
        "  --log_dir={log_dir} \\\n",
        "  --eval_dir={eval_dir} \\\n",
        "  --start_epoch=0 \\\n",
        "  --val_every=5 \\\n",
        "  --max_early_stop_count=30 \\\n",
        "  --max_epoch=280 \\\n",
        "  --data_dicts_json={data_dicts_json} \\\n",
        "  --pin_memory \\\n",
        "  --out_channels=4 \\\n",
        "  --feature_size=48 \\\n",
        "  --drop_rate=0.0 \\\n",
        "  --norm_name='instance' \\\n",
        "  --a_min=-80 \\\n",
        "  --a_max=450 \\\n",
        "  --space_x=0.7 \\\n",
        "  --space_y=0.7 \\\n",
        "  --space_z=1.0 \\\n",
        "  --roi_x=128 \\\n",
        "  --roi_y=128 \\\n",
        "  --roi_z=128 \\\n",
        "  --optim=\"AdamW\" \\\n",
        "  --lr=1e-4 \\\n",
        "  --weight_decay=1e-5 \\\n",
        "  --checkpoint={final_checkpoint} \\\n",
        "  --use_init_weights \\\n",
        "  --infer_post_process\n",
        "\n",
        "print(\"\\n訓練完成!\")"
      ],
      "metadata": {
        "id": "kqutAuJajOHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a83567b3-c53a-4c5f-b0a4-9180484aef79"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "總共 50 組資料\n",
            "   訓練集: 40 個\n",
            "   驗證集: 5 個\n",
            "   測試集: 5 個\n",
            "----------------------------------------------------------------------\n",
            "開始訓練\n",
            "2025-12-07 10:01:15.873151: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-07 10:01:15.892289: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765101675.913906   17691 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765101675.920361   17691 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-12-07 10:01:15.941694: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1086, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1153, in _cache_bytecode\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 582, in _calc_mode\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 147, in _path_stat\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/CardiacSegV2/expers/tune.py\", line 25, in <module>\n",
            "    from monailabel.transform.post import Restored\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/monailabel/transform/post.py\", line 30, in <module>\n",
            "    from shapely.geometry import Point, Polygon\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/shapely/__init__.py\", line 13, in <module>\n",
            "    from shapely.set_operations import *\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1150, in _find_and_load_unlocked\n",
            "KeyboardInterrupt\n",
            "\n",
            "訓練完成!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "0zgwom5_Nir0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 修改 tune.py 並測試 (SwinUNETR 對應版)\n",
        "\n",
        "import os\n",
        "\n",
        "# 修改 tune.py\n",
        "!cp /content/CardiacSegV2/expers/tune.py /content/CardiacSegV2/expers/tune.py.bak\n",
        "\n",
        "with open('/content/CardiacSegV2/expers/tune.py', 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "old_code = \"\"\"if args.tune_mode == 'test':\n",
        "        print('test mode')\"\"\"\n",
        "\n",
        "new_code = \"\"\"if args.tune_mode == 'test':\n",
        "        print('test mode')\n",
        "        if args.checkpoint and os.path.exists(args.checkpoint):\n",
        "            print(f'Direct test mode: using checkpoint {args.checkpoint}')\n",
        "            args.max_epochs = args.max_epoch\n",
        "            args.test_mode = True\n",
        "            main_worker(args)\n",
        "            import sys\n",
        "            sys.exit(0)\"\"\"\n",
        "\n",
        "if old_code in content:\n",
        "    content = content.replace(old_code, new_code)\n",
        "    with open('/content/CardiacSegV2/expers/tune.py', 'w') as f:\n",
        "        f.write(content)\n",
        "    print(\"tune.py 已修改!\")\n",
        "else:\n",
        "    print(\" tune.py 可能已修改過\")\n",
        "\n",
        "# testing\n",
        "print( \"-\" * 70)\n",
        "print(\"開始測試 SwinUNETR v2\")\n",
        "\n",
        "test_checkpoint = '/content/drive/MyDrive/CardiacSeg_Checkpoints_SwinUNETR/best_model.pth'\n",
        "\n",
        "if os.path.exists(test_checkpoint):\n",
        "    size_mb = os.path.getsize(test_checkpoint) / (1024**2)\n",
        "    print(f\"使用模型: {test_checkpoint}\")\n",
        "    print(f\"   大小: {size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(f\"找不到: {test_checkpoint}\")\n",
        "\n",
        "!python /content/CardiacSegV2/expers/tune.py \\\n",
        "  --tune_mode=\"test\" \\\n",
        "  --exp_name=\"AICUP_swinunetr_v2\" \\\n",
        "  --data_name={data_name} \\\n",
        "  --data_dir={data_dir} \\\n",
        "  --root_exp_dir={root_exp_dir} \\\n",
        "  --model_name={model_name} \\\n",
        "  --model_dir={model_dir} \\\n",
        "  --log_dir={log_dir} \\\n",
        "  --eval_dir={eval_dir} \\\n",
        "  --data_dicts_json={data_dicts_json} \\\n",
        "  --pin_memory \\\n",
        "  --out_channels=4 \\\n",
        "  --feature_size=48 \\\n",
        "  --drop_rate=0.0 \\\n",
        "  --norm_name='instance' \\\n",
        "  --a_min=-80 \\\n",
        "  --a_max=450 \\\n",
        "  --space_x=0.7 \\\n",
        "  --space_y=0.7 \\\n",
        "  --space_z=1.0 \\\n",
        "  --roi_x=128 \\\n",
        "  --roi_y=128 \\\n",
        "  --roi_z=128 \\\n",
        "  --optim=\"AdamW\" \\\n",
        "  --lr=1e-4 \\\n",
        "  --weight_decay=1e-5 \\\n",
        "  --checkpoint={test_checkpoint} \\\n",
        "  --infer_post_process \\\n",
        "  --save_eval_csv \\\n",
        "  --test_mode\n",
        "\n",
        "print(\"\\n 測試完成!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRSA6shgnPTI",
        "outputId": "c0a06180-137a-4b7a-a763-8313ea7d2a9b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tune.py 已修改!\n",
            "----------------------------------------------------------------------\n",
            "開始測試 SwinUNETR v2\n",
            "使用模型: /content/drive/MyDrive/CardiacSeg_Checkpoints_SwinUNETR/best_model.pth\n",
            "   大小: 719.04 MB\n",
            "2025-12-07 10:06:01.997180: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-07 10:06:02.015414: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765101962.036643   19072 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765101962.042953   19072 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-12-07 10:06:02.063557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "2025-12-07 10:06:20,419\tINFO worker.py:1636 -- Started a local Ray instance.\n",
            "2025-12-07 10:06:20,613\tINFO packaging.py:520 -- Creating a file package for local directory '/content/CardiacSegV2'.\n",
            "2025-12-07 10:06:20,621\tWARNING packaging.py:394 -- File /content/CardiacSegV2/.git/objects/pack/pack-73cd69a4f3ad439a9096577174a990c31b4a8aa9.pack is very large (17.31MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/content/CardiacSegV2/.git/objects/pack/pack-73cd69a4f3ad439a9096577174a990c31b4a8aa9.pack']})`\n",
            "2025-12-07 10:06:20,899\tINFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_5bfd1d10adcd7762.zip' (37.60MiB) to Ray cluster...\n",
            "2025-12-07 10:06:21,002\tINFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_5bfd1d10adcd7762.zip'.\n",
            "test mode\n",
            "Direct test mode: using checkpoint /content/drive/MyDrive/CardiacSeg_Checkpoints_SwinUNETR/best_model.pth\n",
            "cuda is available\n",
            "model: swinunetr\n",
            "loss: dice ce loss\n",
            "optimzer: AdamW\n",
            "{'lr': 0.0002, 'weight_decay': 2e-05}\n",
            "=> loaded checkpoint '/content/drive/MyDrive/CardiacSeg_Checkpoints_SwinUNETR/best_model.pth' (epoch 491) (bestacc 0.8711313009262085) (early stop count 0)\n",
            "load json from /content/CardiacSegV2/dataset/chgh/AICUP_training.json\n",
            "train files (40): ['patient0043', 'patient0020', 'patient0047', 'patient0009', 'patient0033', 'patient0005', 'patient0006', 'patient0036', 'patient0032', 'patient0025', 'patient0048', 'patient0015', 'patient0010', 'patient0004', 'patient0017', 'patient0019', 'patient0044', 'patient0037', 'patient0049', 'patient0003', 'patient0022', 'patient0013', 'patient0042', 'patient0024', 'patient0027', 'patient0008', 'patient0045', 'patient0026', 'patient0041', 'patient0034', 'patient0023', 'patient0007', 'patient0028', 'patient0035', 'patient0039', 'patient0001', 'patient0038', 'patient0016', 'patient0002', 'patient0050']\n",
            "val files (5): ['patient0021', 'patient0030', 'patient0012', 'patient0029', 'patient0011']\n",
            "test files (5): ['patient0014', 'patient0040', 'patient0031', 'patient0046', 'patient0018']\n",
            "infer data: {'image': '/content/CardiacSegV2/dataset/chgh/training_image/patient0014.nii.gz', 'label': '/content/CardiacSegV2/dataset/chgh/training_label/patient0014_gt.nii.gz'}\n",
            "/usr/local/lib/python3.11/dist-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.\n",
            "  warn_deprecated(argname, msg, warning_category)\n",
            "/usr/local/lib/python3.11/dist-packages/monai/utils/deprecate_utils.py:111: FutureWarning: <class 'monai.transforms.utility.dictionary.AddChanneld'>: Class `AddChanneld` has been deprecated since version 0.8. It will be removed in version 1.3. please use MetaTensor data type and monai.transforms.EnsureChannelFirstd instead with `channel_dim='no_channel'`.\n",
            "  warn_deprecated(obj, msg, warning_category)\n",
            "/usr/local/lib/python3.11/dist-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.transforms.utility.dictionary EnsureChannelFirstd.__init__:meta_keys: Argument `meta_keys` has been deprecated since version 0.9. not needed if image is type `MetaTensor`.\n",
            "  warn_deprecated(argname, msg, warning_category)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "infer time: 7.218759059906006 sec\n",
            "use post process infer\n",
            "/usr/local/lib/python3.11/dist-packages/monai/utils/deprecate_utils.py:111: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. It will be removed in version 1.3. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead with `channel_dim='no_channel'`.\n",
            "  warn_deprecated(obj, msg, warning_category)\n",
            "Confusion_Vals： [[5.657340e+05 3.200200e+04 8.345408e+06 1.811600e+04]\n",
            " [9.115000e+03 2.745000e+03 8.947752e+06 1.648000e+03]\n",
            " [0.000000e+00 1.200000e+01 8.961248e+06 0.000000e+00]]\n",
            "/content/CardiacSegV2/runners/inferer.py:98: RuntimeWarning: invalid value encountered in divide\n",
            "  sensitivity_vals = tp / (tp + fn)\n",
            "infer test time aug:\n",
            "dice: [0.95758414 0.80581707        nan]\n",
            "iou: [0.91862005 0.6747853         nan]\n",
            "Confusion_Vals： [[4.621735e+06 2.677600e+05 6.835059e+07 1.602330e+05]\n",
            " [7.067000e+04 2.597600e+04 7.328434e+07 1.933800e+04]\n",
            " [0.000000e+00 9.600000e+01 7.340022e+07 0.000000e+00]]\n",
            "infer test original:\n",
            "dice: [0.95574635 0.7572218         nan]\n",
            "iou: [0.91524345 0.6092977         nan]\n",
            "sensitivity: [0.96649224 0.78515244        nan]\n",
            "specificity: [0.99609786 0.99964565 0.9999987 ]\n",
            "infer data: {'image': '/content/CardiacSegV2/dataset/chgh/training_image/patient0040.nii.gz', 'label': '/content/CardiacSegV2/dataset/chgh/training_label/patient0040_gt.nii.gz'}\n",
            "infer time: 5.424705743789673 sec\n",
            "use post process infer\n",
            "Confusion_Vals： [[5.3419400e+05 5.9055000e+04 1.3516408e+07 3.0143000e+04]\n",
            " [6.7470000e+03 2.4090000e+03 1.4128721e+07 1.9230000e+03]\n",
            " [0.0000000e+00 6.0000000e+00 1.4139794e+07 0.0000000e+00]]\n",
            "infer test time aug:\n",
            "dice: [0.92294484 0.7569842         nan]\n",
            "iou: [0.85691506 0.60898995        nan]\n",
            "Confusion_Vals： [[3.846496e+06 3.103040e+05 9.476915e+07 1.644770e+05]\n",
            " [4.657100e+04 1.756000e+04 9.901232e+07 1.398400e+04]\n",
            " [0.000000e+00 3.200000e+01 9.909040e+07 0.000000e+00]]\n",
            "infer test original:\n",
            "dice: [0.9418714 0.7470185       nan]\n",
            "iou: [0.89012945 0.5961927         nan]\n",
            "sensitivity: [0.95899326 0.76906943        nan]\n",
            "specificity: [0.99673635 0.9998227  0.9999997 ]\n",
            "infer data: {'image': '/content/CardiacSegV2/dataset/chgh/training_image/patient0031.nii.gz', 'label': '/content/CardiacSegV2/dataset/chgh/training_label/patient0031_gt.nii.gz'}\n",
            "infer time: 2.332192897796631 sec\n",
            "use post process infer\n",
            "Confusion_Vals： [[3.319200e+05 4.109600e+04 7.608018e+06 2.009100e+04]\n",
            " [7.337000e+03 1.629000e+03 7.987503e+06 4.656000e+03]\n",
            " [0.000000e+00 0.000000e+00 8.001125e+06 0.000000e+00]]\n",
            "infer test time aug:\n",
            "dice: [0.9156073 0.7001288       nan]\n",
            "iou: [0.8443503  0.53861403        nan]\n",
            "Confusion_Vals： [[2.7140030e+06 3.4446800e+05 6.2046972e+07 1.6841400e+05]\n",
            " [5.9603000e+04 1.3792000e+04 6.5157908e+07 4.2553000e+04]\n",
            " [0.0000000e+00 0.0000000e+00 6.5273856e+07 0.0000000e+00]]\n",
            "infer test original:\n",
            "dice: [0.9136693  0.67903525        nan]\n",
            "iou: [0.84106    0.51404494        nan]\n",
            "sensitivity: [0.94157195 0.5834508         nan]\n",
            "specificity: [0.99447894 0.9997884  1.        ]\n",
            "infer data: {'image': '/content/CardiacSegV2/dataset/chgh/training_image/patient0046.nii.gz', 'label': '/content/CardiacSegV2/dataset/chgh/training_label/patient0046_gt.nii.gz'}\n",
            "infer time: 5.437637567520142 sec\n",
            "use post process infer\n",
            "Confusion_Vals： [[5.3460100e+05 7.1950000e+04 1.5438313e+07 2.9586000e+04]\n",
            " [6.6270000e+03 2.0940000e+03 1.6063555e+07 2.1740000e+03]\n",
            " [0.0000000e+00 7.0000000e+00 1.6074443e+07 0.0000000e+00]]\n",
            "infer test time aug:\n",
            "dice: [0.9132718 0.7564205       nan]\n",
            "iou: [0.84038657 0.6082607         nan]\n",
            "Confusion_Vals： [[3.5959630e+06 3.9404000e+05 1.0150041e+08 1.5361800e+05]\n",
            " [4.1658000e+04 1.5112000e+04 1.0557210e+08 1.5167000e+04]\n",
            " [0.0000000e+00 4.8000000e+01 1.0564398e+08 0.0000000e+00]]\n",
            "infer test original:\n",
            "dice: [0.9292392  0.73343486        nan]\n",
            "iou: [0.8678309  0.57907397        nan]\n",
            "sensitivity: [0.9590306  0.73309284        nan]\n",
            "specificity: [0.99613285 0.9998569  0.9999995 ]\n",
            "infer data: {'image': '/content/CardiacSegV2/dataset/chgh/training_image/patient0018.nii.gz', 'label': '/content/CardiacSegV2/dataset/chgh/training_label/patient0018_gt.nii.gz'}\n",
            "infer time: 5.44342827796936 sec\n",
            "use post process infer\n",
            "Confusion_Vals： [[5.868910e+05 4.118700e+04 8.545795e+06 3.420700e+04]\n",
            " [1.077100e+04 2.537000e+03 9.191030e+06 3.742000e+03]\n",
            " [4.500000e+01 2.000000e+00 9.207956e+06 7.700000e+01]]\n",
            "infer test time aug:\n",
            "dice: [0.939645  0.7743072 0.5325444]\n",
            "iou: [0.8861608  0.6317302  0.36290324]\n",
            "Confusion_Vals： [[4.8848060e+06 3.0316000e+05 7.0295304e+07 2.7634900e+05]\n",
            " [8.5611000e+04 2.3976000e+04 7.5611136e+07 3.8896000e+04]\n",
            " [3.3700000e+02 4.8000000e+01 7.5758544e+07 6.8600000e+02]]\n",
            "infer test original:\n",
            "dice: [0.9440044  0.73142725 0.4776754 ]\n",
            "iou: [0.8939473  0.5765749  0.31378025]\n",
            "sensitivity: [0.94645596 0.6875999  0.32942328]\n",
            "specificity: [0.99570584 0.999683   0.99999934]\n",
            "\n",
            "eval result:\n",
            "avg tt dice: 0.8226423\n",
            "avg tt iou: 0.71447855\n",
            "avg inf dice: 0.8092642\n",
            "avg inf iou: 0.7002408\n",
            "avg inf sensitivity: 0.80058396\n",
            "avg inf specificity: 0.99852973\n",
            "avg inf time: 5.1713447093963625\n",
            "     patientId  tt_diceC  tt_diceAO  tt_diceCA   tt_iouC  tt_iouAO  tt_iouCA  inf_diceC  inf_diceAO  inf_diceCA  inf_iouC  inf_iouAO  inf_iouCA  inf_sensitivityC  inf_sensitivityAO  inf_sensitivityCA  inf_specificityC  inf_specificityAO  inf_specificityCA  inf_time\n",
            "0  patient0014  0.957584   0.805817        NaN  0.918620  0.674785       NaN   0.955746    0.757222         NaN  0.915243   0.609298        NaN          0.966492           0.785152                NaN          0.996098           0.999646           0.999999  7.218759\n",
            "1  patient0040  0.922945   0.756984        NaN  0.856915  0.608990       NaN   0.941871    0.747019         NaN  0.890129   0.596193        NaN          0.958993           0.769069                NaN          0.996736           0.999823           1.000000  5.424706\n",
            "2  patient0031  0.915607   0.700129        NaN  0.844350  0.538614       NaN   0.913669    0.679035         NaN  0.841060   0.514045        NaN          0.941572           0.583451                NaN          0.994479           0.999788           1.000000  2.332193\n",
            "3  patient0046  0.913272   0.756420        NaN  0.840387  0.608261       NaN   0.929239    0.733435         NaN  0.867831   0.579074        NaN          0.959031           0.733093                NaN          0.996133           0.999857           1.000000  5.437638\n",
            "4  patient0018  0.939645   0.774307   0.532544  0.886161  0.631730  0.362903   0.944004    0.731427    0.477675  0.893947   0.576575    0.31378          0.946456           0.687600           0.329423          0.995706           0.999683           0.999999  5.443428\n",
            "2025-12-07 10:07:56,403\tWARNING session.py:91 -- Session not detected. You should not be calling `report` outside `tuner.fit()` or while using the class API. \n",
            "2025-12-07 10:07:56,403\tWARNING session.py:97 --   File \"/content/CardiacSegV2/expers/tune.py\", line 379, in <module>\n",
            "    main_worker(args)\n",
            "  File \"/content/CardiacSegV2/expers/tune.py\", line 359, in main_worker\n",
            "    tune.report(\n",
            "\n",
            "\n",
            " 測試完成!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 下載到推論路徑"
      ],
      "metadata": {
        "id": "jSSDAchPOja7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 複製 Checkpoint 到 Google Drive\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# 找到訓練產生的 checkpoint\n",
        "train_exp_dir = \"/content/CardiacSegV2/exps/exps/unet3d/chgh/tune_results/AICUP_training_opt_v1_fixed\"\n",
        "\n",
        "# 搜尋所有 .pth 檔案\n",
        "pth_files = glob.glob(f\"{train_exp_dir}/**/*.pth\", recursive=True)\n",
        "\n",
        "print(\"找到的 checkpoint 檔案:\")\n",
        "for f in pth_files:\n",
        "    size_mb = os.path.getsize(f) / (1024**2)\n",
        "    print(f\"  {f} ({size_mb:.2f} MB)\")\n",
        "\n",
        "# 複製到 Drive\n",
        "drive_dir = '/content/drive/MyDrive/CardiacSeg_Checkpoints'\n",
        "os.makedirs(drive_dir, exist_ok=True)\n",
        "\n",
        "for f in pth_files:\n",
        "    filename = os.path.basename(f)\n",
        "    target = os.path.join(drive_dir, filename)\n",
        "    shutil.copy(f, target)\n",
        "    print(f\"\\n✅ 已複製: {filename}\")\n",
        "\n",
        "print(\"✅ Checkpoint 已複製到 Google Drive!\")"
      ],
      "metadata": {
        "id": "y7eDZsrlypXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8fc77f7-46f2-4920-dd1e-8ad22ce2da89"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "找到的 checkpoint 檔案:\n",
            "✅ Checkpoint 已複製到 Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 把雲端模型複製到推論程式碼的路徑\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 來源: 雲端存好的模型\n",
        "source_dir = '/content/drive/MyDrive/CardiacSeg_Checkpoints_SwinUNETR'\n",
        "\n",
        "# 目標: 推論程式碼預期的路徑\n",
        "target_dir = '/content/CardiacSegV2/models'\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "print(\"複製模型到推論路徑\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# 複製 best_model.pth\n",
        "source_best = os.path.join(source_dir, 'best_model.pth')\n",
        "target_best = os.path.join(target_dir, 'best_model.pth')\n",
        "\n",
        "if os.path.exists(source_best):\n",
        "    shutil.copy(source_best, target_best)\n",
        "    size_mb = os.path.getsize(target_best) / (1024**2)\n",
        "    print(f\" best_model.pth 已複製\")\n",
        "    print(f\"   來源: {source_best}\")\n",
        "    print(f\"   目標: {target_best}\")\n",
        "    print(f\"   大小: {size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(f\" 找不到: {source_best}\")\n",
        "\n",
        "\n",
        "print( \"=\" * 70)\n",
        "print(\"完成!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZgprtO0wH86",
        "outputId": "234e3eb8-c342-4b3c-d400-724232fab610"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "複製模型到推論路徑\n",
            "----------------------------------------------------------------------\n",
            " best_model.pth 已複製\n",
            "   來源: /content/drive/MyDrive/CardiacSeg_Checkpoints_SwinUNETR/best_model.pth\n",
            "   目標: /content/CardiacSegV2/models/best_model.pth\n",
            "   大小: 719.04 MB\n",
            "======================================================================\n",
            "完成!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 檢查確認\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 設定 Drive 儲存路徑\n",
        "drive_save_dir = '/content/drive/MyDrive/CardiacSeg_Models'\n",
        "os.makedirs(drive_save_dir, exist_ok=True)\n",
        "\n",
        "# 複製模型到 Drive\n",
        "source_model = '/content/CardiacSegV2/models/best_model.pth'\n",
        "target_model = os.path.join(drive_save_dir, 'best_model.pth')\n",
        "\n",
        "if os.path.exists(source_model):\n",
        "    shutil.copy(source_model, target_model)\n",
        "    print(f\"✓ 模型已儲存到 Google Drive:\")\n",
        "    print(f\"  {target_model}\")\n",
        "\n",
        "    # 顯示檔案大小\n",
        "    size_mb = os.path.getsize(target_model) / (1024**2)\n",
        "    print(f\"  大小: {size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(f\"✗ 找不到模型: {source_model}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV8wVamDdZeb",
        "outputId": "fd9a816c-44fd-4a91-a4e3-bb34d3638d5e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✓ 模型已儲存到 Google Drive:\n",
            "  /content/drive/MyDrive/CardiacSeg_Models/best_model.pth\n",
            "  大小: 719.04 MB\n"
          ]
        }
      ]
    }
  ]
}