# AI Cup 2025 - Cardiac CT Image Segmentation

**Private Leaderboard Score: 0.811015**

æœ¬å°ˆæ¡ˆä½¿ç”¨ SwinUNETR + Optuna è¶…åƒæ•¸å„ªåŒ–åœ¨ AI Cup 2025 å¿ƒè‡Ÿ CT å½±åƒåˆ†å‰²ç«¶è³½çš„å®Œæ•´å¯¦ä½œã€‚

---

## ğŸ“‘ ç›®éŒ„

- [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
- [ç’°å¢ƒå®‰è£é…ç½®](#ç’°å¢ƒå®‰è£é…ç½®)
- [è³‡æ–™æº–å‚™èˆ‡æ ¼å¼](#è³‡æ–™æº–å‚™èˆ‡æ ¼å¼)
- [ç¨‹å¼ç¢¼çµæ§‹](#ç¨‹å¼ç¢¼çµæ§‹)
- [åŸ·è¡Œæµç¨‹](#åŸ·è¡Œæµç¨‹)
- [é‡è¦æ¨¡å¡Šè¼¸å…¥è¼¸å‡º](#é‡è¦æ¨¡å¡Šè¼¸å…¥è¼¸å‡º)
- [é‡ç¾å¯¦é©—çµæœ](#é‡ç¾å¯¦é©—çµæœ)
- [é™¤éŒ¯æŒ‡å¼•](#é™¤éŒ¯æŒ‡å¼•)
- [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### Colab Notebooksï¼ˆæ¨è–¦ï¼‰

**æ‰€æœ‰ç¨‹å¼ç¢¼å¯ç›´æ¥åœ¨ Google Colab åŸ·è¡Œï¼Œç„¡éœ€æœ¬åœ°ç’°å¢ƒé…ç½®ï¼š**

| Notebook | åŠŸèƒ½ | é ä¼°æ™‚é–“ | é€£çµ |
|----------|------|----------|------|
| Swin UNET è¨“ç·´æ•´ç†.ipynb | å®Œæ•´è¨“ç·´æµç¨‹ | 8-12 å°æ™‚ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1hJoVW_R9V2AZV8dxKHIJa5ZZd9ISPoNa?usp=sharing) |
| Swin UNET æ¨è«–æ•´ç†.ipynb | æ¨¡å‹æ¨è«–é æ¸¬ | 30-60 ç§’/æ¡ˆä¾‹ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1D3AxLantrA6Wr_SJmFd25NCCEONk9i64?usp=sharing) |
| Swin optuna_Search.ipynb | Optuna è¶…åƒæ•¸æœå°‹ | 6-15 å°æ™‚ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1KAJwfxDyMvw0OLZB3CLx9hirNRbkR-yr?usp=sharing) |

---

## ğŸ”§ ç’°å¢ƒå®‰è£é…ç½®

### ç¡¬é«”éœ€æ±‚
```
GPU: NVIDIA L4 (40GB) æˆ–æ›´é«˜ç­‰ç´š GPU
è¨˜æ†¶é«”: è‡³å°‘ 16GB RAM
å„²å­˜ç©ºé–“: è‡³å°‘ 50GB å¯ç”¨ç©ºé–“
```

### è»Ÿé«”ç’°å¢ƒ
```
ä½œæ¥­ç³»çµ±: Ubuntu 22.04 LTSï¼ˆColab é è¨­ï¼‰
Python: 3.10
CUDA: 11.8 æˆ–ä»¥ä¸Š
```

### å®‰è£æ­¥é©Ÿ

#### æ–¹æ³• Aï¼šä½¿ç”¨ Colabï¼ˆæ¨è–¦ï¼‰

ç›´æ¥é»æ“Šä¸Šæ–¹çš„ Colab badgeï¼Œç’°å¢ƒæœƒè‡ªå‹•é…ç½®ã€‚

#### æ–¹æ³• Bï¼šæœ¬åœ°å®‰è£
```bash
# 1. å»ºç«‹ Python è™›æ“¬ç’°å¢ƒ
python3.10 -m venv cardiac_env
source cardiac_env/bin/activate  # Linux/Mac
# cardiac_env\Scripts\activate  # Windows

# 2. å‡ç´š pip
pip install --upgrade pip

# 3. å®‰è£ PyTorchï¼ˆCUDA 11.8ï¼‰
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 \
    --extra-index-url https://download.pytorch.org/whl/cu118

# 4. å®‰è£ MONAI èˆ‡ç›¸é—œå¥—ä»¶
pip install monai==1.2.0
pip install ray==2.5.0
pip install optuna
pip install numpy==1.26.4
pip install scikit-learn
pip install ml_collections
pip install gdown==4.6.0
pip install "pydantic<2.0"
pip install nibabel
pip install tensorboard

# 5. é©—è­‰å®‰è£
python -c "import torch; print('PyTorch:', torch.__version__)"
python -c "import monai; print('MONAI:', monai.__version__)"
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"
```

#### æ–¹æ³• Cï¼šä½¿ç”¨ requirements.txt
```bash
pip install -r requirements.txt
```

**requirements.txt å…§å®¹ï¼š**
```
torch==2.1.0
torchvision==0.16.0
torchaudio==2.1.0
monai==1.2.0
ray==2.5.0
optuna
numpy==1.26.4
scikit-learn
ml-collections
gdown==4.6.0
pydantic<2.0
nibabel
tensorboard
pandas
```

---

## ğŸ“¦ è³‡æ–™æº–å‚™èˆ‡æ ¼å¼

### è³‡æ–™é›†çµæ§‹

ç«¶è³½æä¾›çš„è³‡æ–™é›†æ‡‰çµ„ç¹”å¦‚ä¸‹ï¼š
```
dataset/chgh/
â”œâ”€â”€ training_image/          # è¨“ç·´å½±åƒï¼ˆ50 å€‹ .nii.gz æª”æ¡ˆï¼‰
â”‚   â”œâ”€â”€ case_001.nii.gz
â”‚   â”œâ”€â”€ case_002.nii.gz
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ training_label/          # è¨“ç·´æ¨™ç±¤ï¼ˆ50 å€‹ .nii.gz æª”æ¡ˆï¼‰
â”‚   â”œâ”€â”€ case_001_gt.nii.gz  # å¿…é ˆæœ‰ _gt å¾Œç¶´
â”‚   â”œâ”€â”€ case_002_gt.nii.gz
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ AICUP_training.json      # è‡ªå‹•ç”Ÿæˆçš„è³‡æ–™ç´¢å¼•
```

### è¼¸å…¥è³‡æ–™æ ¼å¼

**å½±åƒæ ¼å¼ï¼š**
- æª”æ¡ˆé¡å‹ï¼šNIfTI (`.nii.gz`)
- ç¶­åº¦ï¼š3D (å…¸å‹å¤§å°ç´„ 512Ã—512Ã—100-300)
- è³‡æ–™å‹åˆ¥ï¼š16-bit signed integer
- å¼·åº¦ç¯„åœï¼šHU å€¼ï¼ˆç´„ -1024 åˆ° 3071ï¼‰
- Spacingï¼šè®Šå‹•ï¼ˆå…¸å‹ç´„ 0.6-1.0 mm per voxelï¼‰

**æ¨™ç±¤æ ¼å¼ï¼š**
- æª”æ¡ˆé¡å‹ï¼šNIfTI (`.nii.gz`)
- ç¶­åº¦ï¼šèˆ‡å°æ‡‰å½±åƒç›¸åŒ
- è³‡æ–™å‹åˆ¥ï¼š8-bit unsigned integer
- æ¨™ç±¤å€¼ï¼š
  - `0` = èƒŒæ™¯
  - `1` = Segment_1ï¼ˆå¿ƒè‡Ÿè‚Œè‚‰ï¼‰
  - `2` = Segment_2ï¼ˆä¸»å‹•è„ˆç“£è†œï¼‰
  - `3` = Segment_3ï¼ˆéˆ£åŒ–ï¼Œé¸æ“‡æ€§æ¨™è¨»ï¼‰

### è³‡æ–™ä¸‹è¼‰èˆ‡æ•´ç†

**åœ¨ Colab ä¸­åŸ·è¡Œ**ï¼ˆè©³è¦‹ `01_Training.ipynb`ï¼‰ï¼š
```python
# 1. æ›è¼‰ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 2. è§£å£“ç¸®è³‡æ–™
zip_path = "/content/drive/MyDrive/training_label.zip"
!unzip -q -o "{zip_path}" -d "/content/CardiacSegV2/dataset/chgh"

# 3. æ•´ç†è³‡æ–™çµæ§‹ï¼ˆè‡ªå‹•é…å°å½±åƒèˆ‡æ¨™ç±¤ï¼‰
# è©³è¦‹ Training notebook çš„å®Œæ•´ç¨‹å¼ç¢¼
```

### è‡ªå‹•ç”Ÿæˆè³‡æ–™ç´¢å¼•

ç¨‹å¼æœƒè‡ªå‹•ç”Ÿæˆ `AICUP_training.json`ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json
{
  "train": [
    {
      "image": "/path/to/case_001.nii.gz",
      "label": "/path/to/case_001_gt.nii.gz"
    },
    ...
  ],
  "val": [...],
  "test": [...]
}
```

---

## ğŸ“‚ ç¨‹å¼ç¢¼çµæ§‹
```
SwinUNETR-/
â”‚
â”œâ”€â”€ README.md                              # æœ¬æ–‡ä»¶
â”œâ”€â”€ tune.py                                # æ ¸å¿ƒè¨“ç·´èˆ‡å„ªåŒ–ç¨‹å¼
â”‚
â””â”€â”€ notebooks/
    â”œâ”€â”€ 01_Training.ipynb                 # è¨“ç·´æµç¨‹
    â”œâ”€â”€ 02_Inference.ipynb                # æ¨è«–é æ¸¬
    â””â”€â”€ 03_Hyperparameter_Search.ipynb    # è¶…åƒæ•¸æœå°‹
```

### æ ¸å¿ƒæª”æ¡ˆèªªæ˜

#### `tune.py`

**åŠŸèƒ½**ï¼šæ ¸å¿ƒè¨“ç·´èˆ‡è¶…åƒæ•¸å„ªåŒ–è…³æœ¬

**ä¸»è¦æ¨¡çµ„**ï¼š
1. `main(config, args)` - ä¸»è¨“ç·´å‡½æ•¸
2. `main_worker(args)` - è¨“ç·´åŸ·è¡Œå‡½æ•¸
3. æ”¯æ´å¤šç¨®è¨“ç·´æ¨¡å¼ï¼š
   - `train` - æ¨™æº–è¨“ç·´
   - `optuna_optim` - åŸºç¤è¶…åƒæ•¸å„ªåŒ–
   - `optuna_advanced` - é€²éšè¶…åƒæ•¸å„ªåŒ–
   - `test` - æ¨¡å‹æ¸¬è©¦

**ç›¸ä¾æª”æ¡ˆ**ï¼š
- éœ€æ­é… CardiacSegV2 baseline å°ˆæ¡ˆä½¿ç”¨
- GitHub: https://github.com/kairaun/CardiacSegV2

---

## ğŸ¯ åŸ·è¡Œæµç¨‹

### 1ï¸âƒ£ è¨“ç·´æ¨¡å‹ï¼ˆTrainingï¼‰

#### è¼¸å…¥

- **è³‡æ–™**ï¼š`dataset/chgh/` ç›®éŒ„ä¸‹çš„å½±åƒèˆ‡æ¨™ç±¤
- **é…ç½®**ï¼šè¨“ç·´åƒæ•¸ï¼ˆlearning rate, epochs ç­‰ï¼‰
- **é è¨“ç·´æ¬Šé‡**ï¼ˆé¸ç”¨ï¼‰ï¼šMONAI å®˜æ–¹ SwinUNETR æ¬Šé‡

#### åŸ·è¡Œ
```bash
# åœ¨ Colab ä¸­åŸ·è¡Œ
%cd /content/CardiacSegV2

!python expers/tune.py \
    --tune_mode="train" \
    --exp_name="AICUP_swinunetr_final" \
    --data_name="chgh" \
    --model_name="swinunetr" \
    --data_dir="/content/CardiacSegV2/dataset/chgh" \
    --data_dicts_json="/content/CardiacSegV2/dataset/chgh/AICUP_training.json" \
    --model_dir="/content/CardiacSegV2/models" \
    --log_dir="/content/CardiacSegV2/logs" \
    --start_epoch=0 \
    --max_epoch=280 \
    --val_every=5 \
    --max_early_stop_count=30 \
    --out_channels=4 \
    --feature_size=48 \
    --roi_x=128 --roi_y=128 --roi_z=128 \
    --a_min=-80 --a_max=450 \
    --space_x=0.7 --space_y=0.7 --space_z=1.0 \
    --optim="AdamW" \
    --lr=1e-4 \
    --weight_decay=1e-5 \
    --use_init_weights \
    --pin_memory
```

#### è¼¸å‡º

- **æ¨¡å‹æª”æ¡ˆ**ï¼š
  - `models/best_model.pth` - é©—è­‰é›†æœ€ä½³æ¨¡å‹
  - `models/final_model.pth` - æœ€çµ‚ epoch çš„æ¨¡å‹
  
- **è¨“ç·´æ—¥èªŒ**ï¼š`logs/` ç›®éŒ„
  - TensorBoard æ—¥èªŒ
  - è¨“ç·´/é©—è­‰ loss æ›²ç·š
  - Dice åˆ†æ•¸è¨˜éŒ„

- **æ ¼å¼**ï¼šPyTorch checkpoint (`.pth`)
```python
  {
      'state_dict': model.state_dict(),
      'optimizer': optimizer.state_dict(),
      'epoch': current_epoch,
      'best_acc': best_validation_dice,
      'early_stop_count': count
  }
```

---

### 2ï¸âƒ£ æ¨¡å‹æ¨è«–ï¼ˆInferenceï¼‰

#### è¼¸å…¥

- **æ¨¡å‹**ï¼š`best_model.pth`ï¼ˆè¨“ç·´å®Œæˆçš„æ¨¡å‹ï¼‰
- **æ¸¬è©¦è³‡æ–™**ï¼šæœªæ¨™è¨»çš„ NIfTI å½±åƒ
```
  test_images/
  â”œâ”€â”€ test_001.nii.gz
  â”œâ”€â”€ test_002.nii.gz
  â””â”€â”€ ...
```

#### åŸ·è¡Œ
```bash
# åœ¨ Colab ä¸­åŸ·è¡Œï¼ˆè©³è¦‹ 02_Inference.ipynbï¼‰
!python expers/infer.py \
    --model_name="swinunetr" \
    --checkpoint="/content/models/best_model.pth" \
    --test_data_dir="/content/test_images" \
    --output_dir="/content/predictions" \
    --out_channels=4 \
    --feature_size=48 \
    --roi_x=128 --roi_y=128 --roi_z=128 \
    --a_min=-80 --a_max=450 \
    --space_x=0.7 --space_y=0.7 --space_z=1.0
```

#### è¼¸å‡º

- **é æ¸¬çµæœ**ï¼š
```
  predictions/
  â”œâ”€â”€ test_001_predict.nii.gz
  â”œâ”€â”€ test_002_predict.nii.gz
  â””â”€â”€ ...
```

- **æ ¼å¼**ï¼šNIfTI (`.nii.gz`)
- **ç¶­åº¦**ï¼šèˆ‡è¼¸å…¥å½±åƒç›¸åŒ
- **æ¨™ç±¤å€¼**ï¼š
  - `0` = èƒŒæ™¯
  - `1` = å¿ƒè‡Ÿè‚Œè‚‰
  - `2` = ä¸»å‹•è„ˆç“£è†œ
  - `3` = éˆ£åŒ–

- **æäº¤æ ¼å¼**ï¼šå£“ç¸®ç‚º ZIP æª”æ¡ˆä¸Šå‚³

---

### 3ï¸âƒ£ è¶…åƒæ•¸æœå°‹ï¼ˆHyperparameter Searchï¼‰

#### è¼¸å…¥

- **æœå°‹ç©ºé–“é…ç½®**ï¼š
```python
  {
      'lr': (1e-5, 5e-4),          # å­¸ç¿’ç‡ç¯„åœ
      'weight_decay': (1e-5, 1e-3), # æ¬Šé‡è¡°æ¸›ç¯„åœ
      'feature_size': [48, 96, 128] # ç‰¹å¾µç¶­åº¦é¸é …
  }
```

- **è³‡æ–™**ï¼šèˆ‡è¨“ç·´ç›¸åŒ

#### åŸ·è¡Œ
```bash
!python expers/tune.py \
    --tune_mode="optuna_optim" \
    --exp_name="AICUP_swinunetr_optuna" \
    --max_epoch=80 \
    --val_every=5 \
    --max_early_stop_count=20 \
    [å…¶ä»–åƒæ•¸åŒè¨“ç·´]
```

#### è¼¸å‡º

- **æœ€ä½³é…ç½®**ï¼š`exps/AICUP_swinunetr_optuna/best_config.json`
```json
  {
    "config": {
      "lr": 0.0001234,
      "weight_decay": 0.00001567,
      "feature_size": 48
    },
    "metrics": {
      "val_bst_acc": 0.8234,
      "inf_dice": 0.8156,
      "tt_dice": 0.8110
    },
    "log_dir": "/path/to/logs"
  }
```

- **æ‰€æœ‰è©¦é©—è¨˜éŒ„**ï¼šOptuna è³‡æ–™åº«
- **è¦–è¦ºåŒ–**ï¼šTensorBoard æ›²ç·š

---

## ğŸ” é‡è¦æ¨¡å¡Šè¼¸å…¥è¼¸å‡º

### æ¨¡å¡Š 1ï¼šè³‡æ–™å‰è™•ç†

**è¼¸å…¥**ï¼š
- åŸå§‹ CT å½±åƒï¼ˆNIfTI æ ¼å¼ï¼‰
- å°ºå¯¸è®Šå‹•ï¼ˆå…¸å‹ 512Ã—512Ã—150ï¼‰
- HU å€¼ç¯„åœ [-1024, 3071]

**è™•ç†æµç¨‹**ï¼š
```python
transforms = Compose([
    # 1. å¼·åº¦æ­£è¦åŒ–
    ScaleIntensityRanged(
        keys=["image"],
        a_min=-80,      # HU ä¸‹é™
        a_max=450,      # HU ä¸Šé™
        b_min=0.0,
        b_max=1.0,
        clip=True
    ),
    
    # 2. ç©ºé–“é‡æ¡æ¨£
    Spacingd(
        keys=["image", "label"],
        pixdim=(0.7, 0.7, 1.0),  # ç›®æ¨™ spacing
        mode=("bilinear", "nearest")
    ),
    
    # 3. æ™ºèƒ½è£åˆ‡
    RandCropByPosNegLabeld(
        keys=["image", "label"],
        label_key="label",
        spatial_size=(128, 128, 128),
        pos=1,
        neg=1,
        num_samples=4
    )
])
```

**è¼¸å‡º**ï¼š
- æ­£è¦åŒ–å¾Œçš„å½±åƒå¼µé‡
- å½¢ç‹€ï¼š`(batch, 1, 128, 128, 128)`
- æ•¸å€¼ç¯„åœï¼š[0.0, 1.0]

---

### æ¨¡å¡Š 2ï¼šSwinUNETR æ¨¡å‹

**è¼¸å…¥**ï¼š
- å½±åƒå¼µé‡ï¼š`(batch, 1, H, W, D)`
- å…¸å‹ï¼š`(1, 1, 128, 128, 128)`

**æ¨¡å‹æ¶æ§‹**ï¼š
```python
model = SwinUNETR(
    img_size=(128, 128, 128),
    in_channels=1,
    out_channels=4,
    feature_size=48,
    drop_rate=0.0,
    attn_drop_rate=0.0,
    dropout_path_rate=0.0,
    use_checkpoint=True,
)
```

**è¼¸å‡º**ï¼š
- Logits å¼µé‡ï¼š`(batch, 4, 128, 128, 128)`
- 4 å€‹é€šé“å°æ‡‰ï¼š[èƒŒæ™¯, è‚Œè‚‰, ç“£è†œ, éˆ£åŒ–]

**å¾Œè™•ç†**ï¼š
```python
pred = torch.argmax(logits, dim=1)  # (batch, 128, 128, 128)
```

---

### æ¨¡å¡Š 3ï¼šæå¤±å‡½æ•¸

**è¼¸å…¥**ï¼š
- é æ¸¬ logitsï¼š`(batch, 4, H, W, D)`
- çœŸå¯¦æ¨™ç±¤ï¼š`(batch, 1, H, W, D)`

**é…ç½®**ï¼š
```python
loss = DiceCELoss(
    to_onehot_y=True,
    softmax=True,
    squared_pred=True,
    include_background=False
)
```

**è¼¸å‡º**ï¼š
- æ¨™é‡æå¤±å€¼ï¼ˆDice Loss + Cross-Entropy Lossï¼‰

---

### æ¨¡å¡Š 4ï¼šæ»‘å‹•è¦–çª—æ¨è«–

**è¼¸å…¥**ï¼š
- å®Œæ•´å½±åƒï¼šä»»æ„å°ºå¯¸ï¼ˆå¦‚ 512Ã—512Ã—200ï¼‰
- ROI å°ºå¯¸ï¼š(128, 128, 128)
- é‡ç–Šç‡ï¼š0.5

**è™•ç†**ï¼š
```python
from monai.inferers import sliding_window_inference

output = sliding_window_inference(
    inputs=image,
    roi_size=(128, 128, 128),
    sw_batch_size=1,
    predictor=model,
    overlap=0.5
)
```

**è¼¸å‡º**ï¼š
- å®Œæ•´å½±åƒçš„é æ¸¬ï¼šèˆ‡è¼¸å…¥ç›¸åŒå°ºå¯¸
- è‡ªå‹•è™•ç†é‚Šç•Œå€åŸŸçš„èåˆ

---

### æ¨¡å¡Š 5ï¼šè©•ä¼°æŒ‡æ¨™

**è¼¸å…¥**ï¼š
- é æ¸¬æ¨™ç±¤ï¼š`(batch, num_classes, H, W, D)`
- çœŸå¯¦æ¨™ç±¤ï¼š`(batch, num_classes, H, W, D)`

**è¨ˆç®—**ï¼š
```python
dice_metric = DiceMetric(
    include_background=True,
    reduction="mean",
    get_not_nans=False
)

dice_score = dice_metric(pred, label)
```

**è¼¸å‡º**ï¼š
- æ¯å€‹é¡åˆ¥çš„ Dice ä¿‚æ•¸
- å¹³å‡ Dice ä¿‚æ•¸

---

## ğŸ”„ é‡ç¾å¯¦é©—çµæœ

### å®Œæ•´é‡ç¾æ­¥é©Ÿ

#### Step 1: ç’°å¢ƒæº–å‚™
```bash
# 1. æ‰“é–‹ Colab: https://colab.research.google.com
# 2. é»æ“Š "Open in Colab" badgeï¼ˆ01_Training.ipynbï¼‰
# 3. ç¢ºèª GPU å·²å•Ÿç”¨: Runtime â†’ Change runtime type â†’ GPU (A100)
```

#### Step 2: è³‡æ–™æº–å‚™
```python
# 1. æ›è¼‰ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 2. ä¸Šå‚³ç«¶è³½è³‡æ–™ training_label.zip åˆ° Drive

# 3. åŸ·è¡Œè³‡æ–™æ•´ç†ï¼ˆNotebook ä¸­çš„å®Œæ•´ç¨‹å¼ç¢¼ï¼‰
# è‡ªå‹•å®Œæˆï¼šè§£å£“ç¸®ã€é…å°ã€åˆ†å‰²ã€ç”Ÿæˆ JSON
```

#### Step 3: è¨“ç·´æ¨¡å‹

**ä½¿ç”¨æœ€ä½³è¶…åƒæ•¸**ï¼ˆå·²æœå°‹ç¢ºèªï¼‰ï¼š
```bash
!python expers/tune.py \
    --tune_mode="train" \
    --exp_name="reproduce_0811015" \
    --data_name="chgh" \
    --model_name="swinunetr" \
    --max_epoch=280 \
    --val_every=5 \
    --max_early_stop_count=30 \
    --lr=1e-4 \
    --weight_decay=1e-5 \
    --feature_size=48 \
    --out_channels=4 \
    --roi_x=128 --roi_y=128 --roi_z=128 \
    --a_min=-80 --a_max=450 \
    --space_x=0.7 --space_y=0.7 --space_z=1.0 \
    --optim="AdamW" \
    --use_init_weights \
    --pin_memory
```

**é æœŸçµæœ**ï¼š
- è¨“ç·´æ™‚é–“ï¼š8-12 å°æ™‚ï¼ˆA100 GPUï¼‰
- é©—è­‰ Diceï¼šç´„ 0.80-0.82
- æ¸¬è©¦ Diceï¼šç´„ 0.81

#### Step 4: æ¨¡å‹æ¨è«–
```bash
# 1. ä¸‹è¼‰æ¸¬è©¦è³‡æ–™
# 2. åŸ·è¡Œæ¨è«–ï¼ˆ02_Inference.ipynbï¼‰
# 3. ç”Ÿæˆé æ¸¬çµæœ
# 4. å£“ç¸®ä¸¦ä¸‹è¼‰
```

#### Step 5: æäº¤èˆ‡é©—è­‰
```bash
# 1. ä¸Šå‚³ predictions.zip åˆ°ç«¶è³½å¹³å°
# 2. ç¢ºèª Private Leaderboard åˆ†æ•¸ç´„ 0.811
```

### å¯é‡ç¾æ€§ä¿è­‰

- âœ… **å›ºå®šéš¨æ©Ÿç¨®å­**ï¼š`random_state=42`
- âœ… **å›ºå®šè³‡æ–™åˆ†å‰²**ï¼šä½¿ç”¨ç›¸åŒ JSON
- âœ… **å›ºå®šè¶…åƒæ•¸**ï¼šå·²è¨˜éŒ„æœ€ä½³é…ç½®
- âœ… **å›ºå®šé è¨“ç·´æ¬Šé‡**ï¼šMONAI å®˜æ–¹ç‰ˆæœ¬
- âœ… **å›ºå®šæ¡†æ¶ç‰ˆæœ¬**ï¼šrequirements.txt é–å®šç‰ˆæœ¬

---

## ğŸ› é™¤éŒ¯æŒ‡å¼•

### å¸¸è¦‹éŒ¯èª¤èˆ‡è§£æ±ºæ–¹æ¡ˆ

#### éŒ¯èª¤ 1: CUDA Out of Memory

**ç—‡ç‹€**ï¼š
```
RuntimeError: CUDA out of memory. Tried to allocate XXX MiB
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# æ–¹æ³• 1: æ¸›å° batch size
--batch_size=1

# æ–¹æ³• 2: æ¸›å° ROI å°ºå¯¸
--roi_x=96 --roi_y=96 --roi_z=96

# æ–¹æ³• 3: ä½¿ç”¨ gradient checkpointing
model = SwinUNETR(..., use_checkpoint=True)

# æ–¹æ³• 4: æ¸…ç†è¨˜æ†¶é«”
import gc
import torch
gc.collect()
torch.cuda.empty_cache()
```

---

#### éŒ¯èª¤ 2: æ‰¾ä¸åˆ°é…å°çš„æ¨™ç±¤æª”æ¡ˆ

**ç—‡ç‹€**ï¼š
```
ValueError: ç„¡æ³•é…å°ä»»ä½•æª”æ¡ˆï¼
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# æª¢æŸ¥æª”åæ ¼å¼
# æ­£ç¢ºï¼šcase_001.nii.gz â†’ case_001_gt.nii.gz
# éŒ¯èª¤ï¼šcase_001.nii.gz â†’ case_001.nii.gz

# é‡æ–°å‘½åæ¨™ç±¤æª”æ¡ˆ
import os
for f in os.listdir("training_label/"):
    if not f.endswith("_gt.nii.gz"):
        new_name = f.replace(".nii.gz", "_gt.nii.gz")
        os.rename(f, new_name)
```

---

#### éŒ¯èª¤ 3: è¨“ç·´éç¨‹ä¸­ Loss è®Šæˆ NaN

**ç—‡ç‹€**ï¼š
```
Epoch 10: loss = nan
```

**åŸå› **ï¼š
- å­¸ç¿’ç‡éé«˜
- éˆ£åŒ–é¡åˆ¥æ¬Šé‡éå¤§

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# é™ä½å­¸ç¿’ç‡
--lr=5e-5

# èª¿æ•´æå¤±å‡½æ•¸æ¬Šé‡ï¼ˆé‡å°éˆ£åŒ–ï¼‰
# åœ¨ tune.py ä¸­ä¿®æ”¹ DiceCELoss çš„ class weights
```

---

#### éŒ¯èª¤ 4: è¼‰å…¥é è¨“ç·´æ¬Šé‡å¤±æ•—

**ç—‡ç‹€**ï¼š
```
RuntimeError: Error(s) in loading state_dict
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# ç¢ºèªä½¿ç”¨ MONAI å®˜æ–¹çš„ SwinUNETR
--use_init_weights

# å¦‚æœä»å¤±æ•—ï¼Œæª¢æŸ¥æ¨¡å‹é…ç½®æ˜¯å¦ä¸€è‡´
--feature_size=48  # å¿…é ˆèˆ‡é è¨“ç·´æ¬Šé‡ä¸€è‡´
```

---

#### éŒ¯èª¤ 5: Colab æ–·ç·šå°è‡´è¨“ç·´ä¸­æ–·

**é é˜²æªæ–½**ï¼š
```python
# 1. è‡ªå‹•å‚™ä»½åˆ° Google Drive
checkpoint_dir = "/content/drive/MyDrive/checkpoints"

# 2. ä½¿ç”¨è¼ƒçŸ­çš„é©—è­‰é–“éš”
--val_every=5

# 3. å•Ÿç”¨ Early Stopping
--max_early_stop_count=30

# 4. å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´
--checkpoint="/path/to/final_model.pth"
```

---

### é™¤éŒ¯æª¢æŸ¥æ¸…å–®

åŸ·è¡Œå‰è«‹ç¢ºèªï¼š

- [ ] GPU å¯ç”¨ï¼š`torch.cuda.is_available() == True`
- [ ] è³‡æ–™è·¯å¾‘æ­£ç¢ºï¼šæª¢æŸ¥ `data_dir` èˆ‡ `data_dicts_json`
- [ ] JSON æª”æ¡ˆæœ‰æ•ˆï¼šé©—è­‰é…å°æ•¸é‡èˆ‡æª”æ¡ˆå­˜åœ¨æ€§
- [ ] ç£ç¢Ÿç©ºé–“å……è¶³ï¼šè‡³å°‘ 50GB
- [ ] Google Drive å·²æ›è¼‰ï¼š`/content/drive` å¯å­˜å–
- [ ] ç‰ˆæœ¬ä¸€è‡´ï¼šç¢ºèªå¥—ä»¶ç‰ˆæœ¬èˆ‡ requirements.txt ç›¸ç¬¦

---

### æ—¥èªŒæª¢æŸ¥

**è¨“ç·´æ—¥èªŒä½ç½®**ï¼š
```
logs/
â”œâ”€â”€ events.out.tfevents.xxx  # TensorBoard æ—¥èªŒ
â””â”€â”€ training.log              # æ–‡å­—æ—¥èªŒ
```

**æŸ¥çœ‹ TensorBoard**ï¼š
```python
%load_ext tensorboard
%tensorboard --logdir /content/CardiacSegV2/logs
```

**é—œéµæŒ‡æ¨™æª¢æŸ¥**ï¼š
- è¨“ç·´ loss æ˜¯å¦ä¸‹é™
- é©—è­‰ Dice æ˜¯å¦ä¸Šå‡
- æ˜¯å¦è§¸ç™¼ Early Stopping

---

## â“ å¸¸è¦‹å•é¡Œ

### Q1: è¨“ç·´éœ€è¦å¤šä¹…ï¼Ÿ

**A**: 
- æ¨™æº–è¨“ç·´ï¼ˆ280 epochsï¼‰ï¼š8-12 å°æ™‚ï¼ˆA100 GPUï¼‰
- è¶…åƒæ•¸æœå°‹ï¼ˆ20 trialsï¼‰ï¼š6-15 å°æ™‚
- å–®æ¬¡æ¨è«–ï¼š30-60 ç§’/æ¡ˆä¾‹

---

### Q2: å¯ä»¥ç”¨æ¯” A100 æ›´å°çš„ GPU å—ï¼Ÿ

**A**: å¯ä»¥ï¼Œä½†éœ€è¦èª¿æ•´ï¼š
```python
# V100 (16GB) æˆ– T4 (16GB)
--roi_x=96 --roi_y=96 --roi_z=96  # æ¸›å° ROI
--batch_size=1                     # å–®ä¸€ batch
--sw_batch_size=1                  # æ¨è«– batch size
```

---

### Q3: å¦‚ä½•æŸ¥çœ‹æ¨¡å‹æ•ˆèƒ½ï¼Ÿ

**A**:
```python
# æŸ¥çœ‹é©—è­‰é›† Dice
# åœ¨è¨“ç·´æ—¥èªŒä¸­ï¼š
# Epoch 150: val_dice = 0.8123

# æŸ¥çœ‹æ¸¬è©¦é›†çµæœ
# åŸ·è¡Œå®Œæ•´è©•ä¼°å¾Œæœƒç”Ÿæˆ CSV æª”æ¡ˆ
import pandas as pd
results = pd.read_csv("evals/best_model.csv")
print(results.describe())
```

---

### Q4: å¦‚ä½•èª¿æ•´è¶…åƒæ•¸ï¼Ÿ

**A**: ä½¿ç”¨ Optuna è‡ªå‹•æœå°‹ï¼ˆæ¨è–¦ï¼‰æˆ–æ‰‹å‹•èª¿æ•´ï¼š
```bash
# æ‰‹å‹•èª¿æ•´
--lr=5e-5              # å­¸ç¿’ç‡
--weight_decay=1e-4    # æ¬Šé‡è¡°æ¸›
--feature_size=96      # ç‰¹å¾µç¶­åº¦
--max_epoch=200        # è¨“ç·´è¼ªæ•¸

# è‡ªå‹•æœå°‹ï¼ˆæ¨è–¦ï¼‰
--tune_mode="optuna_optim"
```

---

### Q5: é æ¸¬çµæœæ ¼å¼ä¸æ­£ç¢ºæ€éº¼è¾¦ï¼Ÿ

**A**: ç¢ºèªï¼š
```python
# 1. æª”åæ ¼å¼
# æ­£ç¢ºï¼štest_001_predict.nii.gz
# éŒ¯èª¤ï¼štest_001.nii.gz

# 2. æ¨™ç±¤å€¼
import nibabel as nib
pred = nib.load("test_001_predict.nii.gz")
data = pred.get_fdata()
print(np.unique(data))  # æ‡‰è©²æ˜¯ [0, 1, 2, 3]

# 3. ç¶­åº¦
print(data.shape)  # æ‡‰èˆ‡åŸå§‹å½±åƒç›¸åŒ
```

---

## ğŸ“§ æŠ€è¡“æ”¯æ´

é‡åˆ°å•é¡Œè«‹ï¼š

1. **æª¢æŸ¥æœ¬ README çš„é™¤éŒ¯æŒ‡å¼•**
2. **æŸ¥çœ‹ Notebook ä¸­çš„è©³ç´°è¨»è§£**
3. **åœ¨ GitHub Issues æå•**ï¼š[æäº¤å•é¡Œ](https://github.com/tinghw0927/SwinUNETR-/issues)
4. **åƒè€ƒå®˜æ–¹æ–‡ä»¶**ï¼š
   - MONAI: https://docs.monai.io/
   - Optuna: https://optuna.readthedocs.io/

---

## ğŸ“š åƒè€ƒè³‡æº

- **ç«¶è³½å¹³å°**ï¼šhttps://tbrain.trendmicro.com.tw/Competitions/Details/41
- **MONAI æ–‡ä»¶**ï¼šhttps://docs.monai.io/
- **SwinUNETR è«–æ–‡**ï¼šhttps://arxiv.org/abs/2201.01266
- **Optuna æ–‡ä»¶**ï¼šhttps://optuna.readthedocs.io/

---

## ğŸ“„ æˆæ¬Š

æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT Licenseã€‚ç«¶è³½è³‡æ–™é›†ç‰ˆæ¬Šæ­¸ AI Cup 2025 ä¸»è¾¦å–®ä½æ‰€æœ‰ã€‚

---

## ğŸ™ è‡´è¬

- MONAI åœ˜éšŠ - é†«å­¸å½±åƒæ·±åº¦å­¸ç¿’æ¡†æ¶
- Optuna åœ˜éšŠ - è¶…åƒæ•¸å„ªåŒ–å·¥å…·
- AI Cup 2025 ä¸»è¾¦å–®ä½ - ç«¶è³½å¹³å°èˆ‡è³‡æ–™é›†
- é•·åºšç´€å¿µé†«é™¢ - æä¾›å¿ƒè‡Ÿ CT è³‡æ–™

---

**æœ€å¾Œæ›´æ–°**ï¼š2025-12-09  
**ä½œè€…**ï¼š[ä½ çš„åå­—/åœ˜éšŠåç¨±]  
**è¯çµ¡**ï¼š[ä½ çš„ Email]

---

<p align="center">âš ï¸ æœ¬å°ˆæ¡ˆåƒ…ä¾›å­¸è¡“ç ”ç©¶èˆ‡æ•™è‚²ç”¨é€”ã€‚æ¨¡å‹é æ¸¬çµæœä¸æ‡‰ç›´æ¥ç”¨æ–¼è‡¨åºŠè¨ºæ–·ã€‚</p>
